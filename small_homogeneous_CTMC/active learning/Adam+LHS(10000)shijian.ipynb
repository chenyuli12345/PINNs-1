{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:42.237432Z",
     "iopub.status.busy": "2025-01-02T12:31:42.236988Z",
     "iopub.status.idle": "2025-01-02T12:31:43.189220Z",
     "shell.execute_reply": "2025-01-02T12:31:43.188874Z"
    }
   },
   "outputs": [],
   "source": [
    "#下面这行代码，是为了把自己编写的代码文件当作一共模块导入，这里是把Utilities文件夹中的plotting.py文件当作python的模块导入，对应的是下面的from plotting import newfig, savefig。路径要随着不同设备的系统做相应的修改\n",
    "import sys #导入sys模块。sys模块提供了一些变量和函数，用于与 Python解释器进行交互和访问。例如，sys.path 是一个 Python 在导入模块时会查找的路径列表，sys.argv 是一个包含命令行参数的列表，sys.exit() 函数可以用于退出 Python 程序。导入 sys 模块后，你就可以在你的程序中使用这些变量和函数了。\n",
    "sys.path.insert(0, '../../Utilities/') #在 Python的sys.path列表中插入一个新的路径。sys.path是一个 Python 在导入模块时会查找的路径列表。新的路径'../../Utilities/'相对于当前脚本的路径。当你尝试导入一个模块时，Python 会在 sys.path 列表中的路径下查找这个模块。通过在列表开始位置插入一个路径，你可以让 Python 优先在这个路径下查找模块。这在你需要导入自定义模块或者不在 Python 标准库中的模块时非常有用。\n",
    "\n",
    "import torch\n",
    "#collections是python一个内置模块，提供了一些有用的数据结构\n",
    "from collections import OrderedDict  #这个类是字典dict的一个子类，用于创建有序的字典。普通字典中元素顺序是无序的，在OrderedDict中元素的顺序是有序的，元素的顺序是按照它们被添加到字典中的顺序决定的。\n",
    "\n",
    "from pyDOE import lhs #`pyDOE`是一个Python库，用于设计实验。它提供了一些函数来生成各种设计，如因子设计、拉丁超立方设计等。`lhs`是库中的一个函数，全名为\"Latin Hypercube Sampling\"，拉丁超立方采样。这是一种统计方法，用于生成一个近似均匀分布的多维样本点集。它在参数空间中生成一个非常均匀的样本，这对于高维数值优化问题非常有用，因为它可以更好地覆盖参数空间。\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io #导入了scipy库中的io模块。scipy.io模块包含了一些用于文件输入/输出的函数，例如读取和写入.mat文件（MATLAB格式）\n",
    "from scipy.interpolate import griddata #`scipy.interpolate`是`scipy`库中的一个模块，提供了许多插值工具，用于在给定的离散数据点之间进行插值和拟合。`griddata`是这个模块中的一个函数，用于在无规则的数据点上进行插值。\n",
    "\n",
    "import random\n",
    "\n",
    "import skopt #用于优化问题的库，特别是机器学习中的超参数优化\n",
    "from distutils.version import LooseVersion #distutils是Python的一个标准库，用于构建和安装Python包。LooseVersion是一个类，用于比较版本号\n",
    "\n",
    "\n",
    "from plotting_torch import newfig, savefig #从自定义的plotting_torch.py文件中导入了newfig和savefig函数。这两个函数用于创建和保存图形。这两个函数的定义在plotting_torch.py文件中\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #`mpl_toolkits.axes_grid1`是`matplotlib`库的一个模块，提供了一些高级的工具来控制matplotlib图形中的坐标轴和颜色条。`make_axes_locatable`是模块中的一个函数，用于创建一个可分割的坐标轴。可以在这个坐标轴的四个方向（上、下、左、右）添加新的坐标轴或颜色条。\n",
    "import matplotlib.gridspec as gridspec #是`matplotlib`库的一个模块，用于创建一个网格布局来放置子图。在`matplotlib`中可以创建一个或多个子图（subplot），每个子图都有自己的坐标轴，并可以在其中绘制图形。`gridspec`模块提供了一个灵活的方式来创建和放置子图。\n",
    "import time #一个内置模块，用于处理时间相关的操作。\n",
    "\n",
    "\n",
    "from tqdm import tqdm #一个快速，可扩展的python进度条库，可以在python长循环中添加一个进度提示信息，用户只需要封装任意的迭代器tqdm(iterator)。\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import timeit #用于计时和测量小段代码的执行时间\n",
    "import seaborn as sns  # 导入seaborn库用于绘制密度图\n",
    "import pandas as pd #一个用于数据操作和分析的库，提供了数据结构和数据分析工具，特别是用于处理表格数据（类似于Excel中的数据表）\n",
    "\n",
    "import subprocess\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.190988Z",
     "iopub.status.busy": "2025-01-02T12:31:43.190881Z",
     "iopub.status.idle": "2025-01-02T12:31:43.365545Z",
     "shell.execute_reply": "2025-01-02T12:31:43.364986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs:  2\n",
      "GPU 0: NVIDIA GeForce RTX 4090, Allocated: 0, Reserved: 0\n",
      "GPU 1: NVIDIA GeForce RTX 4090, Allocated: 0, Reserved: 0\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print('Number of available GPUs: ', num_gpus)\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    torch.cuda.set_device(i)\n",
    "    allocated = torch.cuda.memory_allocated()\n",
    "    reserved = torch.cuda.memory_reserved()\n",
    "    print('GPU {}: {}, Allocated: {}, Reserved: {}'.format(i, torch.cuda.get_device_name(i), allocated, reserved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.379690Z",
     "iopub.status.busy": "2025-01-02T12:31:43.379581Z",
     "iopub.status.idle": "2025-01-02T12:31:43.382127Z",
     "shell.execute_reply": "2025-01-02T12:31:43.381725Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1) #设置当前使用的GPU设备。这里设置为1号GPU设备（第二块显卡）。\n",
    "\n",
    "# CUDA support \n",
    "\n",
    "#设置pytorch的设备，代表了在哪里执行张量积算，设备可以是cpu或者cuda（gpu），并将这个做运算的设备对象存储在变量device中，后续张量计算回在这个设备上执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.383593Z",
     "iopub.status.busy": "2025-01-02T12:31:43.383473Z",
     "iopub.status.idle": "2025-01-02T12:31:43.386970Z",
     "shell.execute_reply": "2025-01-02T12:31:43.386788Z"
    }
   },
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    #第一个方法\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__() #调用父类的__init__方法进行初始化\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1 #定义名为depth的属性，表示神经网络的深度，等于层数-1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh #设置激活函数为tanh\n",
    "         \n",
    "        layer_list = list() #定义一个空列表layer_list\n",
    "        for i in range(self.depth - 1):  #循环depth次\n",
    "            #将每一层（全连接层）添加到layer_list中\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            #将每一层的激活函数添加到layer_list中\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "\n",
    "        #循环结束后，将最后一层的线性变换添加到layer_list中（激活函数为softmax）\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        #输出层的激活函数为tanh\n",
    "        layer_list.append(('activation_%d' % (self.depth - 1), torch.nn.Softmax(dim=1)))  \n",
    "\n",
    "        #然后使用OrderedDict将layer_list中的元素转换为有序字典\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers，将layerDict转换为一个神经网络模型，赋值给self.layers\n",
    "        self.layers = torch.nn.Sequential(layerDict) #layers，长度为2*depth，包含了所有的线性变换和激活函数，奇数索引为线性变换(权重矩阵)，偶数索引为激活函数\n",
    "    \n",
    "    #第二个方法，定义了模型的前向传播过程\n",
    "    def forward(self, x):  #接收输入x\n",
    "        out = self.layers(x) #将输入x传入神经网络模型self.layers中，得到输出out\n",
    "        return out #返回输出out\n",
    "    \n",
    "    # # 新增方法，获取最后一个隐藏层的输出\n",
    "    # def hidden_output(self, x):\n",
    "    #     # 遍历每一层，直到最后一个隐藏层\n",
    "    #     for i in range(self.depth - 1):\n",
    "    #         # 获取当前层的线性变换\n",
    "    #         x = self.layers[i*2](x)\n",
    "    #         # 获取当前层的激活函数\n",
    "    #         x = self.layers[i*2 + 1](x)\n",
    "    #     # 返回最后一个隐藏层的输出\n",
    "    #     return x\n",
    "\n",
    "    # 新增方法，循环传递输入\n",
    "    def hidden_forward(self, x, num_cycles):\n",
    "        # 将输入传递到最后一个隐藏层，x为最后一个隐藏层的输出\n",
    "        for i in range(self.depth - 1):\n",
    "            x = self.layers[i*2](x)\n",
    "            x = self.layers[i*2 + 1](x)\n",
    "        \n",
    "        # 将最后一个隐藏层的输出作为输入传递到第一个隐藏层中，重复num_cycles次\n",
    "        for _ in range(num_cycles):\n",
    "            for i in range(1, self.depth - 1): #最开始的x为最后一个隐藏层的输出，然后从第一个隐藏层开始传递\n",
    "                x = self.layers[i*2](x)\n",
    "                x = self.layers[i*2 + 1](x)\n",
    "        hidden_output = x\n",
    "        \n",
    "        return hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.388130Z",
     "iopub.status.busy": "2025-01-02T12:31:43.388056Z",
     "iopub.status.idle": "2025-01-02T12:31:43.394987Z",
     "shell.execute_reply": "2025-01-02T12:31:43.394805Z"
    }
   },
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub): #这个类包含的第一个方法__init__，这是一个特殊的方法，也就是这个类的构造函数，用于初始化新创建的对象，接受了几个参数\n",
    "\n",
    "        mu_x, sigma_x = X_f.mean(0), X_f.std(0) #计算X_u的均值和标准差\n",
    "        X_u = (X_u - mu_x) / sigma_x #对X_u进行标准化\n",
    "        X_f = (X_f - mu_x) / sigma_x #对X_f进行标准化\n",
    "\n",
    "        self.mu_x = torch.tensor(mu_x).float().to(device) #创建一个pytorch张量（数据来源于mu_x），并将其转换为浮点类型，最后将张量移动到指定的设备上\n",
    "        self.sigma_x = torch.tensor(sigma_x).float().to(device) #创建一个pytorch张量（数据来源于sigma_x），并将其转换为浮点类型，最后将张量移动到指定的设备上\n",
    "\n",
    "        # boundary conditions\n",
    "        #将传入的lb和ub参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.lb和self.ub来访问这些值。\n",
    "        self.lb = torch.tensor(lb).float().to(device) #创建一个pytorch张量（数据来源于lb），并将其转换为浮点类型，最后将张量移动到指定的设备上\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        # data\n",
    "        #初边界点数据\n",
    "        self.X_u = torch.tensor(X_u, requires_grad=True).float().to(device)\n",
    "        #初边界点真实数据\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "        #配位点数据\n",
    "        self.X_f = torch.tensor(X_f, requires_grad=True).float().to(device)\n",
    "        # #测试点数据\n",
    "        # self.X_star = torch.tensor(X_star).float().to(device)\n",
    "        # #测试点真实数据\n",
    "        # self.u_star = torch.tensor(u_star).float().to(device)\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device) #创建一个DNN类的实例，传入layers参数来实现神经网络的初始化，然后将这个实例移动到指定的设备上\n",
    "        \n",
    "        # optimizers: using the same settings\n",
    "        #创建优化器，括号内为要优化的参数，使用Adam优化方法\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), eps = 1e-8) \n",
    "\n",
    "\n",
    "        self.iter = 0 #记录迭代次数 \n",
    "\n",
    "        self.loss_value = [] #创建一个空列表，用于存储损失值\n",
    "\n",
    "        self.test_error = [] #创建一个空列表，用于存储测试误差\n",
    "        self.test_error_mse = [] #创建一个空列表，用于存储测试误差\n",
    "        self.test_error_mae = [] #创建一个空列表，用于存储测试误差\n",
    "        \n",
    "    #主动学习更新配位点数据    \n",
    "    def update_data(self, X_u, u, X_f):\n",
    "        mu_x, sigma_x = X_f.mean(0), X_f.std(0)\n",
    "        X_u = (X_u - mu_x) / sigma_x\n",
    "        X_f = (X_f - mu_x) / sigma_x\n",
    "\n",
    "        self.mu_x = torch.tensor(mu_x).float().to(device)\n",
    "        self.sigma_x = torch.tensor(sigma_x).float().to(device)\n",
    "        \n",
    "        self.X_u = torch.tensor(X_u, requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "        self.X_f = torch.tensor(X_f, requires_grad=True).float().to(device)\n",
    "    \n",
    "    \n",
    "    #定义了一个名为net_u的函数/方法，用于计算神经网络的输出。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回神经网络的输出。     \n",
    "    def net_u(self, x):  \n",
    "        u = self.dnn(x)  #（第一个参数将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量）调用DNN，根据两个参数权重和偏置，以及新得到的张量，计算神经网络的输出u\n",
    "        return u\n",
    "    \n",
    "    #定义了一个名为net_f的函数/方法，用于计算论文中的f。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回计算得到的f。\n",
    "    def net_f(self, x):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        u = self.net_u(x) #调用上面的net_u函数，计算神经网络的输出u\n",
    "        u1 = u[:, 0:1] #取u的第一列，对应状态1\n",
    "        u2 = u[:, 1:2] #取u的第二列，对应状态2\n",
    "        u3 = u[:, 2:3] #取u的第三列，对应状态3\n",
    "\n",
    "        u_x_1 = torch.autograd.grad(\n",
    "            u1, x, #输入的张量，要计算u1关于x的导数\n",
    "            grad_outputs=torch.ones_like(u1), #生成一个与u1形状相同，所有元素均为1的张量，这个参数用于指定向量-雅可比积的像两部分\n",
    "            retain_graph=True, #表示计算完梯度之后保留计算图若需要多次计算梯度，则需要设置改参数为True\n",
    "            create_graph=True #创建梯度的计算图，使我们能够计算高阶导数\n",
    "        )[0] #这个函数的返回值是一个元组，其中包含了每个输入张量的梯度。这里只关心第一个输入张量u1的梯度，所以我们使用[0]来获取这个梯度。？？？？又说只有一个梯度\n",
    "        u_x_2 = torch.autograd.grad(\n",
    "            u2, x, #输入的张量，要计算u2关于x的导数\n",
    "            grad_outputs=torch.ones_like(u2), #生成一个与u2形状相同，所有元素均为1的张量，这个参数用于指定向量-雅可比积的像两部分\n",
    "            retain_graph=True, #表示计算完梯度之后保留计算图若需要多次计算梯度，则需要设置改参数为True\n",
    "            create_graph=True #创建梯度的计算图，使我们能够计算高阶导数\n",
    "        )[0] #这个函数的返回值是一个元组，其中包含了每个输入张量的梯度。这里只关心第一个输入张量u2的梯度，所以我们使用[0]来获取这个梯度。？？？？又说只有一个梯度\n",
    "        u_x_3 = torch.autograd.grad(\n",
    "            u3, x, #输入的张量，要计算u3关于x的导数\n",
    "            grad_outputs=torch.ones_like(u3), #生成一个与u3形状相同，所有元素均为1的张量，这个参数用于指定向量-雅可比积的像两部分\n",
    "            retain_graph=True, #表示计算完梯度之后保留计算图若需要多次计算梯度，则需要设置改参数为True\n",
    "            create_graph=True #创建梯度的计算图，使我们能够计算高阶导数\n",
    "        )[0] #这个函数的返回值是一个元组，其中包含了每个输入张量的梯度。这里只关心第一个输入张量u3的梯度，所以我们使用[0]来获取这个梯度。？？？？又说只有一个梯度\n",
    "\n",
    "        #归一化\n",
    "        u_x_1 = u_x_1 / self.sigma_x\n",
    "        u_x_2 = u_x_2 / self.sigma_x\n",
    "        u_x_3 = u_x_3 / self.sigma_x\n",
    "        \n",
    "        #计算残差\n",
    "        residual_1 = u_x_1-(-1.286e-4*u1)\n",
    "        residual_2 = u_x_2-(5.6e-5*u1-1.006e-4*u2)\n",
    "        residual_3 = u_x_3-(7.26e-5*u1+1.006e-4*u2)\n",
    "\n",
    "        loss_1 = torch.mean(residual_1 ** 2)\n",
    "        loss_2 = torch.mean(residual_2 ** 2)\n",
    "        loss_3 = torch.mean(residual_3 ** 2)\n",
    "\n",
    "        loss_f = loss_1 + loss_2 + loss_3\n",
    "        return loss_f, residual_1 + residual_2 + residual_3\n",
    "    \n",
    "      \n",
    "    \n",
    "    def train(self, nIter):\n",
    "\n",
    "\n",
    "        #使用Adam优化器优化nIter次\n",
    "        for epoch in tqdm(range(nIter), desc='Adam'):\n",
    "            self.dnn.train() #将神经网络设置为训练模式而不是评估模式\n",
    "            u_pred = self.net_u(self.X_u) #调用之前定义的函数，传入初值得到神经网络的输出u\n",
    "\n",
    "            loss_u = torch.mean((self.u - u_pred) ** 2) #计算初值的损失函数\n",
    "            loss_f, _ = self.net_f(self.X_f) #计算残差的损失函数\n",
    "            \n",
    "\n",
    "            loss = loss_u + loss_f\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad() #清除该优化器之前计算的梯度（在PyTorch中，梯度会累积，所以在每次新的优化迭代之前，我们需要清除之前的梯度）\n",
    "            loss.backward() #被调用以计算损失函数关于神经网络参数的梯度。这个梯度将被用于优化器来更新神经网络参数\n",
    "            self.optimizer_Adam.step()  #使用之前的优化器self.optimizer_Adam，调用step方法(执行一步优化算法)，传入损失函数self.loss_func，进行优化\n",
    "            \n",
    "            #record the loss value\n",
    "            self.loss_value.append(loss) #将计算得到的loss值添加到self.loss_value列表中\n",
    "\n",
    "            # # record the test error\n",
    "            # self.dnn.eval() #将神经网络切换为评估模式\n",
    "            # with torch.no_grad():\n",
    "            #     u_real_pred = self.net_u(self.x_star, self.t_star) #调用之前定义的函数，传入参数得到神经网络的输出u\n",
    "\n",
    "\n",
    "            # error_test = torch.norm(self.u_star-u_real_pred,2)/torch.norm(self.u_star,2)\n",
    "\n",
    "            # self.test_error.append(error_test)\n",
    "\n",
    "            # # 计算 MAE和MSE\n",
    "            # mae = torch.mean(torch.abs(self.u_star - u_real_pred))\n",
    "            # mse = torch.mean((self.u_star - u_real_pred) ** 2)\n",
    "            # # 记录 MAE 和 MSE\n",
    "            # self.test_error_mae.append(mae)\n",
    "            # self.test_error_mse.append(mse)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device) #从输入中得到x和t（第一列和第二列），是张量，需要计算梯度，转换为浮点数类型，并将张量移动到指定设备上\n",
    "        mu, sigma = self.mu_x, self.sigma_x\n",
    "        x = (x - mu) / sigma\n",
    "\n",
    "        self.dnn.eval() #将神经网络切换为评估模式\n",
    "        u = self.net_u(x) #调用之前定义的函数得到神经网络的输出u,以及f\n",
    "        _, f = self.net_f(x)\n",
    "        u = u.detach().cpu().numpy() #将张量u和f先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f\n",
    "    \n",
    "\n",
    "    def hidden_predict(self, x, num_cycles):\n",
    "        x = torch.tensor(x, requires_grad=True).float().to(device) #从输入中得到x和t（第一列和第二列），是张量，需要计算梯度，转换为浮点数类型，并将张量移动到指定设备上\n",
    "        mu, sigma = self.mu_x, self.sigma_x\n",
    "        x = (x - mu) / sigma\n",
    "\n",
    "        self.dnn.eval()\n",
    "        hidden_output = self.dnn.hidden_forward(x, num_cycles)\n",
    "        hidden_output = hidden_output.detach().cpu().numpy()\n",
    "        return hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.396121Z",
     "iopub.status.busy": "2025-01-02T12:31:43.395986Z",
     "iopub.status.idle": "2025-01-02T12:31:43.399503Z",
     "shell.execute_reply": "2025-01-02T12:31:43.399312Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义采样函数，目的是采用sampler方法，生成n_samples个在指定空间内的准随机样本，这里space为二维的，因此每个样本都是一个二维点，即n_samples*2的数据点\n",
    "\n",
    "#共有6种采样器，分别是LHS、Halton、Hammersley、Sobol、Grid和Random，均为均匀采样方法\n",
    "\n",
    "def caiyang(n_samples, sampler): #接受两个参数，n_samples是样本数量，sampler是采样器名称，是一个字符串\n",
    "    space = [(-1.0, 1.0), (0.0, 1.0)] #指定样本生成的空间，一个二维空间，第一个维度是-1到1，第二个维度是0到1\n",
    "\n",
    "    #根据sampler的不同，选择不同的采样器，返回的sampler是一个采样器对象\n",
    "    if sampler == \"LHS\": #如果采样器是LHS（拉丁超采样，每个维度都被均匀划分为等量区间，每个样本都是从每个维度的一个区间中随机选取的）\n",
    "        sampler = skopt.sampler.Lhs(lhs_type=\"centered\", criterion=\"maximin\", iterations=1000) #第一个参数表示如何从每个区间选取样本，这里表示从每个区间的中心选取样本；第二个参数表示如何划分区间，这里表示尽可能使样本之间的最小距离最大；第三个表示通过优化过程得到样本量的迭代次数（即会尝试1000种不同的样本配置，并最终选择质量最好的那个）\n",
    "    elif sampler == \"Halton\": #Halton序列是一种低差异序列，用于在高维空间中生成点\n",
    "        sampler = skopt.sampler.Halton(min_skip=-1, max_skip=-1)  #两个参数用于控制序列的起始点，Halton序列可以通过跳过序列的前几个点来改变序列的七十点。两个参数分别制定了跳过点的最小和最大数量，这里-1表示不跳过任何点\n",
    "    elif sampler == \"Hammersley\": #Hammersley序列是一种低差异序列，用于在高维空间中生成点\n",
    "        sampler = skopt.sampler.Hammersly(min_skip=-1, max_skip=-1) #两个参数用于控制序列的起始点，Hammersley序列可以通过跳过序列的前几个点来改变序列的七十点。两个参数分别制定了跳过点的最小和最大数量，这里-1表示不跳过任何点\n",
    "    elif sampler == \"Sobol\":\n",
    "        # Remove the first point [0, 0, ...] and the second point [0.5, 0.5, ...], which are too special and may cause some error.\n",
    "        # Sobol采样器的实现有一个问题，即生成的前两个样本点通常不是随机的而是固定的，Sobol序列的前两个点（[0, 0, ...]和[0.5, 0.5, ...]）在许多情况下都被认为是“特殊”的点，可能会对某些计算产生不利影响。因此设置跳过前两个点，而且skopt库在0.9版本号取消了max/min_skip参数，所以需要根据skopt的版本号来选择不同的参数\n",
    "        if LooseVersion(skopt.__version__) < LooseVersion(\"0.9\"): #先检查skopt的版本是否大于0.9,若小于\n",
    "            sampler = skopt.sampler.Sobol(min_skip=2, max_skip=2, randomize=False) #则使用Sobol采样器，min_skip和max_skip表示跳过的点的数量，这里表示跳过前两个点，randomize表示是否随机化\n",
    "        else: #若skopt的版本大于0.9\n",
    "            sampler = skopt.sampler.Sobol(skip=0, randomize=False) #则使用Sobol采样器，skip表示跳过的点的数量，这里表示不跳过任何点，randomize表示是否随机化 \n",
    "            return np.array(sampler.generate(space, n_samples + 2)[2:]) #生成n_samples+2个样本，然后返回除了前两个样本之外的所有样本，也就是返回n_samples个样本，每个样本都是一个二维点，且范围在指定的空间space里面\n",
    "    elif sampler == \"Grid\":\n",
    "        x_min, x_max = space[1]\n",
    "        t_min, t_max = space[0]\n",
    "        \n",
    "        # 计算每个维度的网格大小\n",
    "        x_grid_size = (x_max - x_min) / (n_samples // int(np.sqrt(n_samples)) - 1) # x维度上（纵轴），每行有10个点\n",
    "        t_grid_size = (t_max - t_min) / int(np.sqrt(n_samples))  # \n",
    "        \n",
    "        # 生成等距均匀网格采样点\n",
    "        samples = []\n",
    "        for i in range(n_samples // int(np.sqrt(n_samples))):\n",
    "            for j in range(int(np.sqrt(n_samples))):\n",
    "                # 计算每个网格单元的中心点\n",
    "                x = x_min + i * x_grid_size\n",
    "                t = t_min + j * t_grid_size\n",
    "                samples.append([t, x])\n",
    "        \n",
    "        return np.array(samples)\n",
    "    \n",
    "    elif sampler == \"Random\":\n",
    "        # 从space中提取出x_min, x_max, t_min, t_max\n",
    "        x_min, x_max = space[1]\n",
    "        t_min, t_max = space[0]\n",
    "\n",
    "        # 生成x和t的随机数\n",
    "        x = np.random.rand(n_samples, 1) * (x_max - x_min) + x_min\n",
    "        t = np.random.rand(n_samples, 1) * (t_max - t_min) + t_min\n",
    "\n",
    "        # 将x和t合并为一个(n_samples, 2)的数组\n",
    "        samples = np.hstack((t, x))\n",
    "        return samples #生成一个形状为(n_samples, 2)的随机数组\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(sampler.generate(space, n_samples)) #生成n_samples个样本，每个样本都是一个二维点，且范围在指定的空间space里面（n_samples*2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.400623Z",
     "iopub.status.busy": "2025-01-02T12:31:43.400473Z",
     "iopub.status.idle": "2025-01-02T12:31:43.402546Z",
     "shell.execute_reply": "2025-01-02T12:31:43.402374Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义设置随机数种子的函数，第一个参数seed表示种子；第二个参数用来设置CUDA的卷积操作是否确定性，默认为False，表示没有确定性\n",
    "def set_seed(seed):\n",
    "    # torch.manual_seed(seed) #设置pytorch的CPU随机数生成器的种子\n",
    "    # torch.cuda.manual_seed_all(seed) #设置putorch的所有GPU随机数生成器的种子\n",
    "    # np.random.seed(seed) #设置numpy的随机数生成器的种子\n",
    "    # random.seed(seed) #设置python的内置随机数生成器的种子\n",
    "    # torch.backends.cudnn.deterministic = deterministic #True会让CUDA的卷积操作变得确定性，即对于相同的输入，每次运行会得到相同的结果，False则相反\n",
    "    \"\"\"\n",
    "    设置PyTorch的随机种子, 用于生成随机数. 通过设置相同的种子, 可以确保每次运行时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置PyTorch在所有可用的CUDA设备上的随机种子. 如果在使用GPU进行计算, 这个设置可以确保在不同的GPU上生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置PyTorch在当前CUDA设备上的随机种子. 它与上一行代码的作用类似, 但只影响当前设备\n",
    "    \"\"\"\n",
    "    torch.cuda.manual_seed(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置NumPy的随机种子, 用于生成随机数. 通过设置相同的种子，可以确保在使用NumPy的随机函数时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    设置Python内置的随机函数的种子. Python的random模块提供了许多随机函数, 包括生成随机数、打乱列表等. 通过设置相同的种子, 可以确保使用这些随机函数时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    设置Python的哈希种子 (哈希函数被广泛用于数据结构 (如字典和集合) 的实现，以及一些内部操作 (如查找和比较)). 通过设置相同的种子, 可以确保在不同的运行中生成的哈希结果相同\n",
    "    \"\"\"\n",
    "    # os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    该设置确保每次运行代码时, cuDNN的计算结果是确定性的, 即相同的输入会产生相同的输出, 这是通过禁用一些非确定性的算法来实现的, 例如在卷积操作中使用的算法. 这样做可以保证模型的训练和推理在相同的硬件和软件环境下是可复现的, 即每次运行代码时的结果都相同. 但是, 这可能会导致一些性能上的损失, 因为禁用了一些优化的非确定性算法\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \"\"\"\n",
    "    该设置禁用了cuDNN的自动优化过程. 当它被设置为False时, PyTorch不会在每次运行时重新寻找最优的算法配置, 而是使用固定的算法配置. 这样做可以确保每次运行代码时的性能是一致的, 但可能会导致一些性能上的损失\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.403584Z",
     "iopub.status.busy": "2025-01-02T12:31:43.403513Z",
     "iopub.status.idle": "2025-01-02T12:31:43.406624Z",
     "shell.execute_reply": "2025-01-02T12:31:43.406443Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义根据模型计算给定输入（点集中的点）的混沌度的函数，这个函数接受三个参数，分别是模型、输入数据和迭代次数\n",
    "def calculate_chaos(model, X, num_iter):\n",
    "    \"\"\"\n",
    "    计算模型混沌情况的函数。\n",
    "    参数:\n",
    "    - model: 用于预测的模型对象，必须有一个名为hidden_predict的方法。模型的hidden_predict为倒数第二层的输出，倒数第二层的维度必须和输入维度相同。\n",
    "    - X: 输入数据，形状为(N_f_new, 2)，其中第一列为x0，第二列为t0。\n",
    "    - num_iter: 计算混沌情况的迭代次数。\n",
    "    返回:\n",
    "    - distances: 每个采样点（与该采样点加上微扰比较）在最后一次迭代后的欧氏距离数组，形状为(N_f_new,)。\n",
    "    \"\"\"\n",
    "    #对于所有的采样点\n",
    "    x0 = X[:, 0:1] #取X_f_train_new的第一列，赋值给x0，(N_f_new,1)形状\n",
    "    t0 = X[:, 1:2] #取X_f_train_new的第二列，赋值给t0\n",
    "    # 利用x0和t0计算x{t}和t{t}，存储在xs中\n",
    "    xs = [] #初始化xs\n",
    "    x,t = model.hidden_predict(x0,t0) #调用predict方法，传入X_f_train_new，得到x和t，这里x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "    x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "    t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "    \n",
    "    # 迭代预测\n",
    "    for i in range(num_iter): #循环num_iter次\n",
    "        x,t = model.hidden_predict(x,t) #每次计算隐藏层输出，得到的x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "        x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "        t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "        xs.append([x,t]) #将x的数据添加到xs中\n",
    "    #最后得到的xs是一个列表，列表中的每个元素都是一个列表（num_iter个元素），每个列表中有两个元素，分别代表x和t，长度均为N_f_new，对应原始采样点的迭代结果\n",
    "\n",
    "\n",
    "    # 给所有采样点加上一个很小的扰动\n",
    "    x1 = x0 + np.random.normal(0, 0.0001) #加上一个很小的扰动，(N_f_new,1)形状\n",
    "    t1 = t0 + np.random.normal(0, 0.0001)\n",
    "    # 利用x0{1}和t0{1}计算x{t1}和t{t1}，存储在xs1中\n",
    "    xs1 = [] #初始化xs1\n",
    "    x,t = model.hidden_predict(x1,t1) #调用predict方法，传入X_f_train_new，得到x和t，这里x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "    x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "    t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "\n",
    "    # 迭代预测（扰动后）\n",
    "    for i in range(num_iter): #循环num_iter次\n",
    "        x,t = model.hidden_predict(x,t) #每次计算隐藏层输出，得到的x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "        x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)\n",
    "        t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)\n",
    "        xs1.append([x,t]) #将x的数据添加到xs1中\n",
    "    #最后得到的xs1是一个列表，列表中的每个元素都是一个列表（num_iter个元素），每个列表中有两个元素，分别代表x和t，长度均为N_f_new，对应加了扰动后的采样点的迭代结果\n",
    "\n",
    "    # 计算最后一次迭代的隐藏层输出，即最后一次迭代的x和t\n",
    "    last_iter_xs = np.array(xs[-1]) #转换为数组，便于之后计算距离\n",
    "    last_iter_xs1 = np.array(xs1[-1])\n",
    "    #这两个数组的形状均为(2,N_f_new,1)，第一个代表x和t，第二个代表N_f_new个样本点得到的结果，第三个代表1个数\n",
    "\n",
    "    # 计算这两个点的欧氏距离\n",
    "    distances = np.linalg.norm(last_iter_xs - last_iter_xs1, axis=0)\n",
    "    #得到的是一个形状为（N_f_new,1）的数组，每个元素代表了两个点之间的欧氏距离，这里点在xt平面上\n",
    "\n",
    "    distances = distances.flatten()\n",
    "\n",
    "    #对distances进行归一化\n",
    "    # distances = distances / np.linalg.norm(distances)\n",
    "    # 对distances进行归一化前，检查分母是否接近零\n",
    "    # norm = np.linalg.norm(distances)\n",
    "    # if norm < 1e-10:  # 1e-10是一个非常小的数，用于检测norm是否接近于零\n",
    "    #     distances = np.zeros_like(distances)  # 如果分母接近0，将distances设置为全零数组，因为范数为0时，distances中的值想对于彼此几乎没有差异，意味着所有点都几乎处于同一混沌度水平\n",
    "    # else:\n",
    "    #     distances = distances / norm\n",
    "\n",
    "    # 现在可以安全地根据distances对点进行排序，即使在所有值都相同的情况下\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.407547Z",
     "iopub.status.busy": "2025-01-02T12:31:43.407438Z",
     "iopub.status.idle": "2025-01-02T12:31:43.409855Z",
     "shell.execute_reply": "2025-01-02T12:31:43.409680Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定期采样GPU使用情况的函数\n",
    "def sample_gpu_usage(interval, gpu_usage_list, stop_event, gpu_index): #interval是采样间隔时间\n",
    "    while not stop_event.is_set():\n",
    "        gpu_usage = subprocess.check_output(\n",
    "            ['nvidia-smi', '-i', str(gpu_index), '--query-gpu=utilization.gpu,memory.used', '--format=csv,noheader,nounits']\n",
    "        ).decode('utf-8').strip().split('\\n')[0].split(', ')\n",
    "        gpu_usage = [int(x) for x in gpu_usage]\n",
    "        gpu_usage_list.append(gpu_usage)\n",
    "        time.sleep(interval)\n",
    "\n",
    "# 模拟训练函数\n",
    "def train_model(model, Iter, sample_interval=1, gpu_index=1): #这里gpu_index代表GPU的索引，sample_interval代表采样间隔时间几s\n",
    "    # 用于存储GPU使用情况的列表\n",
    "    gpu_usage_list = []\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # 启动一个线程定期采样GPU使用情况\n",
    "    sampling_thread = threading.Thread(target=sample_gpu_usage, args=(sample_interval, gpu_usage_list, stop_event, gpu_index))\n",
    "    sampling_thread.start()\n",
    "\n",
    "    start_time = time.time()  # 获取当前时间\n",
    "    # 训练模型\n",
    "    model.train(Iter)\n",
    "    end_time = time.time() # 获取当前时间\n",
    "    training_time = end_time - start_time # 计算训练时间\n",
    "\n",
    "    # 停止采样线程\n",
    "    stop_event.set()\n",
    "    sampling_thread.join()\n",
    "\n",
    "    # 计算平均GPU使用情况\n",
    "    avg_gpu_usage = np.mean(gpu_usage_list, axis=0)\n",
    "\n",
    "    return training_time, avg_gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:31:43.410794Z",
     "iopub.status.busy": "2025-01-02T12:31:43.410707Z",
     "iopub.status.idle": "2025-01-02T12:50:18.741422Z",
     "shell.execute_reply": "2025-01-02T12:50:18.740026Z"
    }
   },
   "outputs": [],
   "source": [
    "#basic PINN\n",
    "#RAR-G方法，对1000个点，先选择10个点训练500次，然后每500次迭代重采样100个点，选出其中残差最大的10个点添加到训练点中；最后总共有1000个点，共训练10000次\n",
    "seeds = [0, 1, 12, 33, 123, 321, 1234, 4321, 12345, 54321] #生成10个随机种子\n",
    "# seeds = [0]\n",
    "#设置噪声水平为0\n",
    "noise = 0.0        \n",
    "\n",
    "#读取真实解\n",
    "eigenvector_solution_df = pd.read_csv('analytical_solution_by_eigenvector_method.csv')\n",
    "eigenvector_solution_df.columns = ['state_0', 'state_1', 'state_2']\n",
    "\n",
    "N_u = 1\n",
    "N_f = 10000\n",
    "\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [1, 50, 50, 3]\n",
    "\n",
    "lb  = np.array([0.0])              # t的左边界\n",
    "ub  = np.array([60000.0])          # t的右边界\n",
    "\n",
    "# Define the initial MSS state\n",
    "X_u = np.array([[0]])\n",
    "u = np.array([[1,0,0]])          # 初始真实状态\n",
    "# X_f = np.linspace(lb, ub, N_f)     # 配位点位置，0~60000的5000个点\n",
    "\n",
    "PINN_training_time = [] #创建一个空列表，用于存储PINN训练时间\n",
    "GPU_usage = [] #创建一个空列表，用于存储GPU使用率\n",
    "GPU_memory = [] #创建一个空列表，用于存储GPU显存占用\n",
    "\n",
    "i = 0 #初始化i为0\n",
    "\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed) #设置随机数种子\n",
    "\n",
    "    X_f = lb + (ub - lb) * lhs(1, N_f) #生成N_f个随机点\n",
    "    nIter = 80000 #设置迭代次数为10000\n",
    "\n",
    "    #创建PINN模型并输入各种参数     \n",
    "    model = PhysicsInformedNN(X_u, u, X_f, layers, lb, ub)\n",
    "    #开始训练\n",
    "\n",
    "\n",
    "    training_time, avg_gpu_usage = train_model(model, Iter = nIter, sample_interval=1)\n",
    "    print(f\"训练时间: {training_time:.2f} 秒\")\n",
    "    print(f\"平均GPU使用率: {avg_gpu_usage[0]:.2f}%\")\n",
    "    print(f\"平均GPU显存使用量: {avg_gpu_usage[1]:.2f}MiB\")\n",
    "\n",
    "    PINN_training_time.append(training_time)\n",
    "    GPU_usage.append(avg_gpu_usage[0])\n",
    "    GPU_memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    i+=1 #i加1\n",
    "    print(f'当前为第{i}次循环，种子为{seed}')\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:50:18.746654Z",
     "iopub.status.busy": "2025-01-02T12:50:18.746101Z",
     "iopub.status.idle": "2025-01-02T12:50:18.754651Z",
     "shell.execute_reply": "2025-01-02T12:50:18.753688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN训练时间为['111.31816172599792%', '108.17176628112793%', '111.023202419281%', '111.3482506275177%', '108.8461844921112%', '109.4990291595459%', '109.4466381072998%', '110.73111653327942%', '108.43356728553772%', '109.00051522254944%']s\n",
      "平均PINN训练时间为109.78s\n",
      "GPU使用率为['50.09345794392523%', '51.46153846153846%', '50.046728971962615%', '49.570093457943926%', '50.76190476190476%', '51.0188679245283%', '50.839622641509436%', '50.242990654205606%', '51.076190476190476%', '50.923809523809524%']\n",
      "平均GPU使用率为50.60%\n",
      "GPU显存使用为['589.2803738317757MiB', '609.0MiB', '609.0MiB', '609.0MiB', '609.0MiB', '609.0MiB', '609.0MiB', '609.0MiB', '609.0MiB', '609.0MiB']\n",
      "平均GPU显存使用为607.03MiB\n"
     ]
    }
   ],
   "source": [
    "#普通PINN\n",
    "# 打印PINN训练时间\n",
    "print(f'PINN训练时间为{[f\"{traingtime}%\" for traingtime in PINN_training_time]}s')\n",
    "# 打印平均PINN训练时间\n",
    "print(f'平均PINN训练时间为{np.mean(PINN_training_time):.2f}s')\n",
    "\n",
    "# 打印GPU使用率\n",
    "print(f'GPU使用率为{[f\"{usage}%\" for usage in GPU_usage]}')\n",
    "# 打印平均GPU使用率\n",
    "print(f'平均GPU使用率为{np.mean(GPU_usage):.2f}%')\n",
    "\n",
    "# 打印GPU显存使用率\n",
    "print(f'GPU显存使用为{[f\"{memory}MiB\" for memory in GPU_memory]}')\n",
    "# 打印平均GPU显存使用率\n",
    "print(f'平均GPU显存使用为{np.mean(GPU_memory):.2f}MiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:50:18.759161Z",
     "iopub.status.busy": "2025-01-02T12:50:18.758670Z",
     "iopub.status.idle": "2025-01-02T12:50:19.058027Z",
     "shell.execute_reply": "2025-01-02T12:50:19.057586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "效率为216.96\n"
     ]
    }
   ],
   "source": [
    "#计算效率=总训练时间/平均GPU使用率\n",
    "efficiency = 109.78 / 0.5060\n",
    "print(f'效率为{efficiency:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总训练次数为 8.00e+08\n"
     ]
    }
   ],
   "source": [
    "#总训练次数\n",
    "N_f = 10000\n",
    "nIter = 80000\n",
    "total_training = N_f * nIter\n",
    "\n",
    "# 用科学计数法表示\n",
    "total_training_scientific = f\"{total_training:.2e}\"\n",
    "print(f'总训练次数为 {total_training_scientific}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:50:19.059757Z",
     "iopub.status.busy": "2025-01-02T12:50:19.059646Z",
     "iopub.status.idle": "2025-01-02T13:25:29.053330Z",
     "shell.execute_reply": "2025-01-02T13:25:29.051619Z"
    }
   },
   "outputs": [],
   "source": [
    "#RAR-G方法，对1000个点，先选择10个点训练500次，然后每500次迭代重采样100个点，选出其中残差最大的10个点添加到训练点中；最后总共有1000个点，共训练10000次\n",
    "seeds = [0, 1, 12, 33, 123, 321, 1234, 4321, 12345, 54321] #生成10个随机种子\n",
    "# seeds = [0]\n",
    "#设置噪声水平为0\n",
    "noise = 0.0        \n",
    "\n",
    "#读取真实解\n",
    "eigenvector_solution_df = pd.read_csv('analytical_solution_by_eigenvector_method.csv')\n",
    "eigenvector_solution_df.columns = ['state_0', 'state_1', 'state_2']\n",
    "\n",
    "N_u = 1\n",
    "N_f = 10000\n",
    "\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [1, 50, 50, 3]\n",
    "\n",
    "lb  = np.array([0.0])              # t的左边界\n",
    "ub  = np.array([60000.0])          # t的右边界\n",
    "\n",
    "# Define the initial MSS state\n",
    "X_u = np.array([[0]])\n",
    "u = np.array([[1,0,0]])          # 初始真实状态\n",
    "# X_f = np.linspace(lb, ub, N_f)     # 配位点位置，0~60000的5000个点\n",
    "\n",
    "\n",
    "PINN_training_time = [] #创建一个空列表，用于存储PINN训练时间\n",
    "GPU_usage = [] #创建一个空列表，用于存储GPU使用率\n",
    "GPU_memory = [] #创建一个空列表，用于存储GPU显存占用\n",
    "\n",
    "\n",
    "i = 0 #初始化i为0\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed) #设置随机数种子\n",
    "\n",
    "    timeused = [] #创建一个空列表，用于存储时间\n",
    "    Usage = [] #创建一个空列表，用于存储使用率\n",
    "    Memory = [] #创建一个空列表，用于存储显存占用\n",
    "\n",
    "\n",
    "    N_f_1 = N_f // 100 #先拿50个点训练\n",
    "    X_f_train = lb + (ub - lb) * lhs(1, N_f_1) #生成N_f个随机点\n",
    "    nIter = 80000 #设置迭代次数为10000\n",
    "\n",
    "    #创建PINN模型并输入各种参数     \n",
    "    model = PhysicsInformedNN(X_u, u, X_f_train, layers, lb, ub)\n",
    "\n",
    "\n",
    "    training_time, avg_gpu_usage = train_model(model, nIter//100)\n",
    "\n",
    "    timeused.append(training_time)\n",
    "    Usage.append(avg_gpu_usage[0])\n",
    "    Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "\n",
    "    for iter in range(nIter//100+1, nIter+1, nIter//100): #每800次迭代\n",
    "        N_f_new = N_f_1 * 10 #重新采样500个点\n",
    "        X_f_train_new = lb + (ub - lb) * lhs(1, N_f_new) #生成N_f_new个随机点\n",
    "\n",
    "        #计算残差\n",
    "        _, residual = model.predict(X_f_train_new)\n",
    "        abs_residual = np.abs(residual)\n",
    "        abs_residual = abs_residual.flatten()\n",
    "        #对abs_residual进行归一化\n",
    "        abs_residual = abs_residual / np.linalg.norm(abs_residual)\n",
    "\n",
    "\n",
    "        #计算混沌度\n",
    "        y_chaos = model.hidden_predict(X_f_train_new, 100)\n",
    "        x_ssss = X_f_train_new + np.random.normal(0, 0.001)\n",
    "        y_chaos_ssss = model.hidden_predict(x_ssss, 100)\n",
    "        chaos = np.linalg.norm(y_chaos - y_chaos_ssss,axis=1)\n",
    "        #对chaos进行归一化\n",
    "        epsilon = 1e-10\n",
    "        norm_chaos = np.linalg.norm(chaos)\n",
    "        if norm_chaos > epsilon:\n",
    "            chaos = chaos / norm_chaos\n",
    "        else:\n",
    "            chaos = np.zeros_like(chaos)\n",
    "\n",
    "        #计算信息量\n",
    "        xinxi = abs_residual + chaos\n",
    "\n",
    "        #选出信息量最大的10个点\n",
    "        # 找出绝对值最大的10个值的索引\n",
    "        topk_indices = np.argpartition(xinxi, -N_f_1)[-N_f_1:] #该函数会对数组进行排序，使得指定的k个最大值出现在数组的最后k给位置上，并获取最后1000个元素\n",
    "\n",
    "        # 使用这些索引来提取对应的数据\n",
    "        X_f_train_topk = X_f_train_new[topk_indices]\n",
    "\n",
    "        X_f_train = np.vstack((X_f_train, X_f_train_topk)) #与之前的训练数据合并\n",
    "\n",
    "        # 更新模型中的X_f_train数据\n",
    "        model.update_data(X_u, u, X_f_train)\n",
    "\n",
    "        # 在更新数据后的模型上进行训练500次\n",
    "        # model.train(nIter//100,0)\n",
    "        training_time, avg_gpu_usage = train_model(model, nIter//100)\n",
    "\n",
    "        timeused.append(training_time)\n",
    "        Usage.append(avg_gpu_usage[0])\n",
    "        Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    # print(X_f_train.shape)\n",
    "    # 更新模型中的X_f_train数据\n",
    "    model.update_data(X_u, u, X_f_train)\n",
    "\n",
    "\n",
    "    training_time = sum(timeused)\n",
    "    avg_gpu_usage = [np.mean(Usage), np.mean(Memory)]\n",
    "    print(f\"训练时间: {training_time:.2f} 秒\")\n",
    "    print(f\"平均GPU使用率: {avg_gpu_usage[0]:.2f}%\")\n",
    "    print(f\"平均GPU显存使用: {avg_gpu_usage[1]:.2f}MiB\")\n",
    "\n",
    "    PINN_training_time.append(training_time)\n",
    "    GPU_usage.append(avg_gpu_usage[0])\n",
    "    GPU_memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    \n",
    "\n",
    "    i+=1 #i加1\n",
    "    print(f'当前为第{i}次循环，种子为{seed}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T13:25:29.058583Z",
     "iopub.status.busy": "2025-01-02T13:25:29.058178Z",
     "iopub.status.idle": "2025-01-02T13:25:29.067118Z",
     "shell.execute_reply": "2025-01-02T13:25:29.065801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN训练时间为['115.80553531646729%', '114.29670834541321%', '115.17911338806152%', '115.38664174079895%', '114.25273942947388%', '115.7835795879364%', '117.80080366134644%', '115.51872372627258%', '115.38215708732605%', '116.0473701953888%']s\n",
      "平均PINN训练时间为115.55s\n",
      "GPU使用率为['22.53%', '22.62%', '22.625%', '22.235%', '23.195%', '22.57%', '22.41%', '22.925%', '22.555%', '22.735%']\n",
      "平均GPU使用率为22.64%\n",
      "GPU显存使用为['615.84MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB']\n",
      "平均GPU显存使用为627.68MiB\n"
     ]
    }
   ],
   "source": [
    "#fuhe PINN\n",
    "# 打印PINN训练时间\n",
    "print(f'PINN训练时间为{[f\"{traingtime}%\" for traingtime in PINN_training_time]}s')\n",
    "# 打印平均PINN训练时间\n",
    "print(f'平均PINN训练时间为{np.mean(PINN_training_time):.2f}s')\n",
    "\n",
    "# 打印GPU使用率\n",
    "print(f'GPU使用率为{[f\"{usage}%\" for usage in GPU_usage]}')\n",
    "# 打印平均GPU使用率\n",
    "print(f'平均GPU使用率为{np.mean(GPU_usage):.2f}%')\n",
    "\n",
    "# 打印GPU显存使用率\n",
    "print(f'GPU显存使用为{[f\"{memory}MiB\" for memory in GPU_memory]}')\n",
    "# 打印平均GPU显存使用率\n",
    "print(f'平均GPU显存使用为{np.mean(GPU_memory):.2f}MiB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T13:25:29.072289Z",
     "iopub.status.busy": "2025-01-02T13:25:29.071910Z",
     "iopub.status.idle": "2025-01-02T13:25:29.462437Z",
     "shell.execute_reply": "2025-01-02T13:25:29.462073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "效率为510.38\n"
     ]
    }
   ],
   "source": [
    "#计算效率=总训练时间/平均GPU使用率\n",
    "efficiency = 115.55 / 0.2264\n",
    "print(f'效率为{efficiency:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总训练次数为 4.04e+08\n"
     ]
    }
   ],
   "source": [
    "#总训练次数\n",
    "def calculate_total_training(N_f, nIter):\n",
    "    total_training = 0\n",
    "    current_points = N_f // 100\n",
    "    iterations_per_stage = nIter // 100\n",
    "\n",
    "    for stage in range(1, 101):\n",
    "        total_training += current_points * iterations_per_stage\n",
    "        current_points += N_f // 100\n",
    "\n",
    "    return total_training\n",
    "\n",
    "\n",
    "total_training = calculate_total_training(N_f, nIter)\n",
    "\n",
    "# 用科学计数法表示\n",
    "total_training_scientific = f\"{total_training:.2e}\"\n",
    "print(f'总训练次数为 {total_training_scientific}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T13:25:29.463884Z",
     "iopub.status.busy": "2025-01-02T13:25:29.463782Z",
     "iopub.status.idle": "2025-01-02T14:00:42.391433Z",
     "shell.execute_reply": "2025-01-02T14:00:42.390166Z"
    }
   },
   "outputs": [],
   "source": [
    "#RAR-G方法，对1000个点，先选择10个点训练500次，然后每500次迭代重采样100个点，选出其中残差最大的10个点添加到训练点中；最后总共有1000个点，共训练10000次\n",
    "seeds = [0, 1, 12, 33, 123, 321, 1234, 4321, 12345, 54321] #生成10个随机种子\n",
    "# seeds = [0]\n",
    "#设置噪声水平为0\n",
    "noise = 0.0        \n",
    "\n",
    "#读取真实解\n",
    "eigenvector_solution_df = pd.read_csv('analytical_solution_by_eigenvector_method.csv')\n",
    "eigenvector_solution_df.columns = ['state_0', 'state_1', 'state_2']\n",
    "\n",
    "N_u = 1\n",
    "N_f = 10000\n",
    "\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [1, 50, 50, 3]\n",
    "\n",
    "lb  = np.array([0.0])              # t的左边界\n",
    "ub  = np.array([60000.0])          # t的右边界\n",
    "\n",
    "# Define the initial MSS state\n",
    "X_u = np.array([[0]])\n",
    "u = np.array([[1,0,0]])          # 初始真实状态\n",
    "\n",
    "\n",
    "PINN_training_time = [] #创建一个空列表，用于存储PINN训练时间\n",
    "GPU_usage = [] #创建一个空列表，用于存储GPU使用率\n",
    "GPU_memory = [] #创建一个空列表，用于存储GPU显存占用\n",
    "\n",
    "\n",
    "i = 0 #初始化i为0\n",
    "\n",
    "nIter = 50000 #设置迭代次数为10000\n",
    "nIterLBFGS = 10000 #设置LBFGS迭代次数为500\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed) #设置随机数种子\n",
    "\n",
    "    timeused = [] #创建一个空列表，用于存储时间\n",
    "    Usage = [] #创建一个空列表，用于存储使用率\n",
    "    Memory = [] #创建一个空列表，用于存储显存占用\n",
    "\n",
    "    # X_f = lb + (ub-lb)*lhs(2, N_f) #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（$N_f$，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本$X_f$。\n",
    "\n",
    "    #先训练500次\n",
    "    #采样配位点10个\n",
    "    N_f_1 = N_f // 100 #先拿50个点训练\n",
    "    X_f = lb + (ub - lb) * lhs(1, N_f*2) #生成N_f个随机点\n",
    "    indices = np.arange(X_f.shape[0])\n",
    "    id = np.random.choice(indices, N_f_1, replace=False) #从indices中随机选择10个数，replace=False表示不允许重复选择，最后将这10个数赋值给id，代表训练过的数据索引\n",
    "    X_f_train = X_f[id, :] #从X_f_train中选取id对应的的10行，赋值给X_f_train\n",
    "\n",
    "\n",
    "    nIter = 80000 #设置迭代次数为10000\n",
    "\n",
    "    #创建PINN模型并输入各种参数     \n",
    "    model = PhysicsInformedNN(X_u, u, X_f_train, layers, lb, ub)\n",
    "\n",
    "    #开始训练模型            \n",
    "    # model.train(nIter//100,0)\n",
    "    training_time, avg_gpu_usage = train_model(model, nIter//100)\n",
    "\n",
    "    timeused.append(training_time)\n",
    "    Usage.append(avg_gpu_usage[0])\n",
    "    Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    #删除已经训练过的数据的索引\n",
    "    indices = np.setdiff1d(indices, id) #从indices中去除id中的元素，最后将结果赋值给indices\n",
    "\n",
    "\n",
    "    for iter in range(nIter//100+1, nIter+1, nIter//100): #每800次迭代\n",
    "        N_f_new = N_f_1 #重新采样500个点\n",
    "        # 生成新的X_f_train数据\n",
    "        id = np.random.choice(indices, N_f_new, replace=False) #从indices中随机选择N_f_new个数，replace=False表示不允许重复选择，最后将这N_f_new个数赋值给id，代表训练过的数据索引\n",
    "\n",
    "        X_f_train_new = X_f[id, :] #从X_f_train中选取id对应的的N_f_new行，赋值给X_f_train_new\n",
    "\n",
    "        X_f_train = np.vstack((X_f_train, X_f_train_new)) #与之前的训练数据合并\n",
    "\n",
    "        # 更新模型中的X_f_train数据\n",
    "        model.update_data(X_u, u, X_f_train)\n",
    "\n",
    "        # 在更新数据后的模型上进行训练500次\n",
    "        # model.train(nIter//100,0)\n",
    "        training_time, avg_gpu_usage = train_model(model, nIter//100)\n",
    "\n",
    "        timeused.append(training_time)\n",
    "        Usage.append(avg_gpu_usage[0])\n",
    "        Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "        # 删除已经训练过的数据的索引\n",
    "        indices = np.setdiff1d(indices, id)\n",
    "\n",
    "\n",
    "    # print(X_f_train.shape)\n",
    "    # 更新模型中的X_f_train数据\n",
    "    model.update_data(X_u, u, X_f_train)\n",
    "\n",
    "    timeused.append(training_time)\n",
    "    Usage.append(avg_gpu_usage[0])\n",
    "    Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    training_time = sum(timeused)\n",
    "    avg_gpu_usage = [np.mean(Usage), np.mean(Memory)]\n",
    "    print(f\"训练时间: {training_time:.2f} 秒\")\n",
    "    print(f\"平均GPU使用率: {avg_gpu_usage[0]:.2f}%\")\n",
    "    print(f\"平均GPU显存使用: {avg_gpu_usage[1]:.2f}MiB\")\n",
    "\n",
    "    PINN_training_time.append(training_time)\n",
    "    GPU_usage.append(avg_gpu_usage[0])\n",
    "    GPU_memory.append(avg_gpu_usage[1])\n",
    "\n",
    "\n",
    "    i+=1 #i加1\n",
    "    print(f'当前为第{i}次循环，种子为{seed}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:00:42.394504Z",
     "iopub.status.busy": "2025-01-02T14:00:42.394404Z",
     "iopub.status.idle": "2025-01-02T14:00:42.397266Z",
     "shell.execute_reply": "2025-01-02T14:00:42.396496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN训练时间为['119.36159658432007%', '118.14625597000122%', '119.59995436668396%', '117.33338332176208%', '121.50091004371643%', '118.17867636680603%', '120.28675770759583%', '118.87422966957092%', '117.73236179351807%', '117.60365605354309%']s\n",
      "平均PINN训练时间为118.86s\n",
      "GPU使用率为['22.485148514851485%', '22.777227722772277%', '22.836633663366335%', '22.965346534653467%', '21.945544554455445%', '22.643564356435643%', '22.485148514851485%', '22.435643564356436%', '22.737623762376238%', '22.797029702970296%']\n",
      "平均GPU使用率为22.61%\n",
      "GPU显存使用为['629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB', '629.0MiB']\n",
      "平均GPU显存使用为629.00MiB\n"
     ]
    }
   ],
   "source": [
    "#acitve PINN\n",
    "\n",
    "# 打印PINN训练时间\n",
    "print(f'PINN训练时间为{[f\"{traingtime}%\" for traingtime in PINN_training_time]}s')\n",
    "# 打印平均PINN训练时间\n",
    "print(f'平均PINN训练时间为{np.mean(PINN_training_time):.2f}s')\n",
    "\n",
    "# 打印GPU使用率\n",
    "print(f'GPU使用率为{[f\"{usage}%\" for usage in GPU_usage]}')\n",
    "# 打印平均GPU使用率\n",
    "print(f'平均GPU使用率为{np.mean(GPU_usage):.2f}%')\n",
    "\n",
    "# 打印GPU显存使用率\n",
    "print(f'GPU显存使用为{[f\"{memory}MiB\" for memory in GPU_memory]}')\n",
    "# 打印平均GPU显存使用率\n",
    "print(f'平均GPU显存使用为{np.mean(GPU_memory):.2f}MiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T14:00:42.402604Z",
     "iopub.status.busy": "2025-01-02T14:00:42.401959Z",
     "iopub.status.idle": "2025-01-02T14:00:42.677304Z",
     "shell.execute_reply": "2025-01-02T14:00:42.677065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "效率为525.70\n"
     ]
    }
   ],
   "source": [
    "#计算效率=总训练时间/平均GPU使用率\n",
    "efficiency = 118.86 / 0.2261\n",
    "print(f'效率为{efficiency:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总训练次数为 4.04e+08\n"
     ]
    }
   ],
   "source": [
    "#总训练次数\n",
    "def calculate_total_training(N_f, nIter):\n",
    "    total_training = 0\n",
    "    current_points = N_f // 100\n",
    "    iterations_per_stage = nIter // 100\n",
    "\n",
    "    for stage in range(1, 101):\n",
    "        total_training += current_points * iterations_per_stage\n",
    "        current_points += N_f // 100\n",
    "\n",
    "    return total_training\n",
    "\n",
    "\n",
    "total_training = calculate_total_training(N_f, nIter)\n",
    "\n",
    "# 用科学计数法表示\n",
    "total_training_scientific = f\"{total_training:.2e}\"\n",
    "print(f'总训练次数为 {total_training_scientific}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
