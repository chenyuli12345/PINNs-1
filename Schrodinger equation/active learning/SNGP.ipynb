{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面这行代码，是为了把自己编写的代码文件当作一共模块导入，这里是把Utilities文件夹中的plotting.py文件当作python的模块导入，对应的是下面的from plotting import newfig, savefig。路径要随着不同设备的系统做相应的修改\n",
    "import sys #导入sys模块。sys模块提供了一些变量和函数，用于与 Python解释器进行交互和访问。例如，sys.path 是一个 Python 在导入模块时会查找的路径列表，sys.argv 是一个包含命令行参数的列表，sys.exit() 函数可以用于退出 Python 程序。导入 sys 模块后，你就可以在你的程序中使用这些变量和函数了。\n",
    "sys.path.insert(0, '../../Utilities/') #在 Python的sys.path列表中插入一个新的路径。sys.path是一个 Python 在导入模块时会查找的路径列表。新的路径'../../Utilities/'相对于当前脚本的路径。当你尝试导入一个模块时，Python 会在 sys.path 列表中的路径下查找这个模块。通过在列表开始位置插入一个路径，你可以让 Python 优先在这个路径下查找模块。这在你需要导入自定义模块或者不在 Python 标准库中的模块时非常有用。\n",
    "\n",
    "import torch\n",
    "#collections是python一个内置模块，提供了一些有用的数据结构\n",
    "from collections import OrderedDict  #这个类是字典dict的一个子类，用于创建有序的字典。普通字典中元素顺序是无序的，在OrderedDict中元素的顺序是有序的，元素的顺序是按照它们被添加到字典中的顺序决定的。\n",
    "\n",
    "from pyDOE import lhs #`pyDOE`是一个Python库，用于设计实验。它提供了一些函数来生成各种设计，如因子设计、拉丁超立方设计等。`lhs`是库中的一个函数，全名为\"Latin Hypercube Sampling\"，拉丁超立方采样。这是一种统计方法，用于生成一个近似均匀分布的多维样本点集。它在参数空间中生成一个非常均匀的样本，这对于高维数值优化问题非常有用，因为它可以更好地覆盖参数空间。\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io #导入了scipy库中的io模块。scipy.io模块包含了一些用于文件输入/输出的函数，例如读取和写入.mat文件（MATLAB格式）\n",
    "from scipy.interpolate import griddata #`scipy.interpolate`是`scipy`库中的一个模块，提供了许多插值工具，用于在给定的离散数据点之间进行插值和拟合。`griddata`是这个模块中的一个函数，用于在无规则的数据点上进行插值。\n",
    "\n",
    "import random\n",
    "\n",
    "import skopt #用于优化问题的库，特别是机器学习中的超参数优化\n",
    "from distutils.version import LooseVersion #distutils是Python的一个标准库，用于构建和安装Python包。LooseVersion是一个类，用于比较版本号\n",
    "\n",
    "\n",
    "from plotting_torch import newfig, savefig #从自定义的plotting_torch.py文件中导入了newfig和savefig函数。这两个函数用于创建和保存图形。这两个函数的定义在plotting_torch.py文件中\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #`mpl_toolkits.axes_grid1`是`matplotlib`库的一个模块，提供了一些高级的工具来控制matplotlib图形中的坐标轴和颜色条。`make_axes_locatable`是模块中的一个函数，用于创建一个可分割的坐标轴。可以在这个坐标轴的四个方向（上、下、左、右）添加新的坐标轴或颜色条。\n",
    "import matplotlib.gridspec as gridspec #是`matplotlib`库的一个模块，用于创建一个网格布局来放置子图。在`matplotlib`中可以创建一个或多个子图（subplot），每个子图都有自己的坐标轴，并可以在其中绘制图形。`gridspec`模块提供了一个灵活的方式来创建和放置子图。\n",
    "import time #一个内置模块，用于处理时间相关的操作。\n",
    "\n",
    "\n",
    "from tqdm import tqdm #一个快速，可扩展的python进度条库，可以在python长循环中添加一个进度提示信息，用户只需要封装任意的迭代器tqdm(iterator)。\n",
    "\n",
    "import math\n",
    "import pandas as pd #pandas用于处理结构化数据\n",
    "from scipy.io import savemat #导入sys模块。sys模块提供了一些变量和函数，用于与 Python解释器进行交互和访问。例如，sys.path 是一个 Python 在导入模块时会查找的路径列表，sys.argv 是一个包含命令行参数的列表，sys.exit() 函数可以用于退出 Python 程序。导入 sys 模块后，你就可以在你的程序中使用这些变量和函数了。\n",
    "from mpl_toolkits.mplot3d import Axes3D #`mpl_toolkits.mplot3d`是`matplotlib`库的一个模块，用于创建三维图形。`Axes3D`是`mpl_toolkits.mplot3d`模块中的一个类，用于创建一个三维的坐标轴。可以在这个坐标轴上绘制三维的图形，如曲线、曲面等。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs:  1\n",
      "GPU 0: NVIDIA A100-SXM4-40GB, Allocated: 0, Reserved: 0\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print('Number of available GPUs: ', num_gpus)\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    torch.cuda.set_device(i)\n",
    "    allocated = torch.cuda.memory_allocated()\n",
    "    reserved = torch.cuda.memory_reserved()\n",
    "    print('GPU {}: {}, Allocated: {}, Reserved: {}'.format(i, torch.cuda.get_device_name(i), allocated, reserved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0) #设置当前使用的GPU设备。这里设置为1号GPU设备（第二块显卡）。\n",
    "\n",
    "# CUDA support \n",
    "\n",
    "#设置pytorch的设备，代表了在哪里执行张量积算，设备可以是cpu或者cuda（gpu），并将这个做运算的设备对象存储在变量device中，后续张量计算回在这个设备上执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#神经网络：layers代表一个正常的神经网络，在输出层和隐藏层之间插入一个高斯过程层\n",
    "\n",
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    #第一个方法\n",
    "    def __init__(self, layers,gamma,num_of_GP): #初始化函数，layers是一个列表，表示每一层的神经元个数，gamma是GP的参数，num_of_GP是GP的参数\n",
    "        super(DNN, self).__init__() #调用父类的__init__方法进行初始化\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1 #定义名为depth的属性，表示神经网络的深度，等于层数-1\n",
    "        self.num_of_GP = num_of_GP #定义名为num_of_GP的属性，表示GP的参数\n",
    "        self.gamma = gamma #定义名为gamma的属性，表示GP的参数\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh #设置激活函数为tanh\n",
    "         \n",
    "        layer_list = list() #定义一个空列表layer_list\n",
    "        for i in range(self.depth - 1):  #循环depth次\n",
    "            #将每一层（全连接层）添加到layer_list中\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            #将每一层的激活函数添加到layer_list中\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "\n",
    "        #循环结束后，将最后一层的线性变换添加到layer_list中（因为没有激活函数了）\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(self.num_of_GP, layers[-1], bias=False))\n",
    "        )\n",
    "        #然后使用OrderedDict将layer_list中的元素转换为有序字典\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers，将layerDict转换为一个神经网络模型，赋值给self.layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()  \n",
    "\n",
    "        # # Initialize beta for the output layer\n",
    "        # self.beta = torch.nn.Parameter(torch.randn(layers[-1], 1))  # 初始化回归权重矩阵beta\n",
    "\n",
    "        # Initialize GP_W and GP_b\n",
    "        self.GP_W = torch.randn(layers[-2], self.num_of_GP) * math.sqrt(2 * self.gamma)  # 初始化GP_W\n",
    "        self.GP_b = torch.rand(1, self.num_of_GP) * 2 * math.pi  # 初始化GP_b\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for idx, m in enumerate(self.layers): #遍历神经网络模型的每一层，获取每一层的索引idx和层对象m(注意这里idx会包括激活函数，也占一个idx)\n",
    "            if isinstance(m, torch.nn.Linear): #判断层对象m是否是全连接层\n",
    "\n",
    "                # 自定义初始化函数\n",
    "                fan_in = m.weight.size(0)  # 输入单元数量\n",
    "                fan_out = m.weight.size(1)  # 输出单元数量\n",
    "                std = (2 / (fan_in + fan_out)) ** 0.5  # 计算标准差\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    m.weight.normal_(0, std)  # 使用自定义标准差初始化权重\n",
    "                if m.bias is not None: #若存在偏置\n",
    "                    torch.nn.init.zeros_(m.bias) #使用常数初始化方法初始化偏置，初始化为0\n",
    "                \n",
    "                # 归一化权重\n",
    "                if idx > 0 and idx < len(self.layers) - 2: #不归一化输入层、输出层和最后一个隐藏层\n",
    "                #注意这里其实idx包含了激活函数，例如对于[1,50,50,50,50,1]，这里共有6层，但是idx应该是9，因为四个隐藏层+四个激活函数+输出层。所以因为idx0是全连接层，最后一个idx为输出层，因此这样做是可以剔除第一个权重和最后一个权重的\n",
    "                    with torch.no_grad(): #确保归一化过程中不进行梯度计算\n",
    "                        norm = m.weight.norm(2) #计算该层权重的二范数\n",
    "                        if norm > 0.99: #若二范数大于0.99\n",
    "                            m.weight.mul_(0.99 / norm) #将权重乘以0.99/norm，使得二范数等于0.99\n",
    "    \n",
    "    #第二个方法，前向传播\n",
    "    def forward(self, X, lb, ub):  # 接收输入x\n",
    "        device = X.device  # 获取输入张量的设备\n",
    "        self.GP_W = self.GP_W.to(device)  # 将GP_W移动到相同设备\n",
    "        self.GP_b = self.GP_b.to(device)  # 将GP_b移动到相同设备\n",
    "        H = 2.0*(X - lb)/(ub - lb) - 1.0 #这里H是X经过归一化处理后的结果，将X映射到了[-1,1]区间内\n",
    "        #第一层\n",
    "        H = self.layers[0](H)\n",
    "        H = self.layers[1](H)\n",
    "\n",
    "        # 中间隐藏层\n",
    "        for i in range(2, len(self.layers) - 2, 2):  # 循环遍历中间隐藏层\n",
    "            with torch.no_grad():\n",
    "                norm = self.layers[i].weight.norm(2)\n",
    "                if norm > 0.99:\n",
    "                    self.layers[i].weight.mul_(0.99 / norm)\n",
    "            # H = self.layers[i+1](self.layers[i](H))\n",
    "            H_1 = self.layers[i+1](self.layers[i](H))  # 计算输出并加上偏置(i+1是激活函数，i是全连接层)\n",
    "            # H = H + H_1  # 加上前一层的输出构成本层新的输出\n",
    "            # H = H.clone() + H_1\n",
    "\n",
    "        # 在最后一个隐藏层和输出层之间添加一层\n",
    "        H1 = torch.matmul(H, self.GP_W) + self.GP_b  # 计算H1\n",
    "        Fai = math.sqrt(2 / self.num_of_GP) * torch.cos(H1)  # 计算Fai\n",
    "        H = Fai  # 更新H为Fai\n",
    "\n",
    "        # 输出层\n",
    "        out = self.layers[-1](H)  # 计算输出\n",
    "        return out  # 返回输出out\n",
    "    \n",
    "    #第三个方法，获取hidden输出\n",
    "    def dnn_for_hidden_features(self, X, lb, ub):  # 接收输入x\n",
    "        H = 2.0*(X - lb)/(ub - lb) - 1.0 #这里H是X经过归一化处理后的结果，将X映射到了[-1,1]区间内\n",
    "        #第一层\n",
    "        H = self.layers[0](H)\n",
    "        H = self.layers[1](H)\n",
    "\n",
    "        # 中间隐藏层\n",
    "        for i in range(2, len(self.layers) - 2, 2):  # 循环遍历中间隐藏层\n",
    "            with torch.no_grad():\n",
    "                norm = self.layers[i].weight.norm(2)\n",
    "                if norm > 0.99:\n",
    "                    self.layers[i].weight.mul_(0.99 / norm)\n",
    "            # H = self.layers[i+1](self.layers[i](H))\n",
    "            H_1 = self.layers[i+1](self.layers[i](H))  # 计算输出并加上偏置(i+1是激活函数，i是全连接层)\n",
    "            # H = H + H_1  # 加上前一层的输出构成本层新的输出\n",
    "            # H = H.clone() + H_1 # 加上前一层的输出构成本层新的输出\n",
    "\n",
    "        # 在最后一个隐藏层和输出层之间添加一层\n",
    "        H1 = torch.matmul(H, self.GP_W) + self.GP_b  # 计算H1\n",
    "        Fai = math.sqrt(2 / self.num_of_GP) * torch.cos(H1)  # 计算Fai\n",
    "\n",
    "        return H, Fai  # 返回输出out\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # # 新增方法，获取最后一个隐藏层的输出\n",
    "    # def hidden_output(self, x):\n",
    "    #     # 遍历每一层，直到最后一个隐藏层\n",
    "    #     for i in range(self.depth - 1):\n",
    "    #         # 获取当前层的线性变换\n",
    "    #         x = self.layers[i*2](x)\n",
    "    #         # 获取当前层的激活函数\n",
    "    #         x = self.layers[i*2 + 1](x)\n",
    "    #     # 返回最后一个隐藏层的输出\n",
    "    #     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the class of PINN\n",
    "\n",
    "#定义了一个名为`PhysicsInformedNN'的类，用于实现基于物理的神经网络。\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub, X_star, u_star, v_star, h_star, gamma, num_of_GP): #这个类包含的第一个方法__init__，这是一个特殊的方法，也就是这个类的构造函数，用于初始化新创建的对象，接受了几个参数\n",
    "        \n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.num_of_GP = num_of_GP\n",
    "        \n",
    "\n",
    "\n",
    "        #`numpy.concatenate`是一个用于数组拼接的函数。它可以将多个数组沿指定的轴拼接在一起，形成一个新的数组：numpy.concatenate((a1,a2, ...), axis=0)其中，`a1,a2, ...`是需要拼接的数组（只能接受数组或序列类型的参数，且参数形状必须相同），可以是多个。`axis`参数用于指定拼接的轴向，`axis=0`表示沿着第一个轴（即行）进行拼接，不指定`axis`参数默认值是0。\n",
    "        X0 = np.concatenate((x0,0*x0), 1) # [x0, 0],将x0和0*x0两个数组在第二个维度（即列）上进行了合并。0*x0会生成一个与x0形状相同，但所有元素都为0的数组。因此，X0的结果是一个新的二维数组，其中第一列是x0的值，第二列全为0\n",
    "        X_lb = np.concatenate((0*tb+lb[0],tb), 1) # [lb[0], tb],将0*tb+lb[0]和tb两个数组在第二个维度（即列）上进行了合并。0*tb+lb[0]会生成一个与tb形状相同，但所有元素都为lb[0]的数组。因此，X_lb的结果是一个新的二维数组，其中第一列全为lb[0]的值，第二列是tb的值。\n",
    "        X_ub = np.concatenate((0*tb+ub[0],tb), 1) # [ub[0], tb],同上生成一个与tb形状相同，但所有元素都为ub[0]的数组。因此，X_ub的结果是一个新的二维数组，其中第一列全为ub[0]的值，第二列是tb的值\n",
    "        \n",
    "        #Python使用self关键字来表示类的实例。当在类的方法中定义一个变量时，例如lb和ub，这些变量只在该方法内部可见，也就是说它们的作用域仅限于该方法。当方法执行完毕后，这些变量就会被销毁，无法在其他方法中访问它们。但如果希望在类的其他方法中也能访问这些变量就需要将它们保存为类的实例属性。这就是self.lb和self.ub的作用。\n",
    "            #通过将lb和ub赋值给self.lb和self.ub，就可以在类的其他方法中通过self.lb和self.ub来访问这些值。总的来说，self.lb和self.ub是类的实例属性，它们的作用域是整个类，而不仅仅是定义它们的方法。\n",
    "        self.lb = torch.tensor(lb).float().to(device) #将传入的lb和ub参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.lb和self.ub来访问这些值。\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "\n",
    "        self.x0 = torch.tensor(X0[:,0:1], requires_grad=True).float().to(device) #将X0的第一列赋值给self.x0（:表示取所有行,0：1实际上表示取第一列，因为python是左闭右开的）,将X0的第二列赋值给self.t0。这样可以在类的其他方法中通过self.x0和self.t0来访问这些值。\n",
    "        self.t0 = torch.tensor(X0[:,1:2], requires_grad=True).float().to(device) #将x0的第二列赋值给self.t0\n",
    "\n",
    "        self.x_lb = torch.tensor(X_lb[:,0:1], requires_grad=True).float().to(device) #将X_lb的第一列赋值给self.x_lb\n",
    "        self.t_lb = torch.tensor(X_lb[:,1:2], requires_grad=True).float().to(device) #将X_lb的第二列赋值给self.t_lb\n",
    "\n",
    "        self.x_ub = torch.tensor(X_ub[:,0:1], requires_grad=True).float().to(device) #将X_ub的第一列赋值给self.x_ub\n",
    "        self.t_ub = torch.tensor(X_ub[:,0:1], requires_grad=True).float().to(device) #将X_ub的第二列赋值给self.t_ub\n",
    "        \n",
    "        self.x_f = torch.tensor(X_f[:,0:1], requires_grad=True).float().to(device) #将X_f的第一列赋值给self.x_f\n",
    "        self.t_f = torch.tensor(X_f[:,1:2], requires_grad=True).float().to(device) #将X_f的第二列赋值给self.t_f\n",
    "        \n",
    "        self.u0 = torch.tensor(u0).float().to(device) #将传入的u0和v0参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.u0和self.v0来访问这些值。\n",
    "        self.v0 = torch.tensor(v0).float().to(device)\n",
    "\n",
    "\n",
    "        self.x_star = torch.tensor(X_star[:,0:1], requires_grad=True).float().to(device) #将X_star的第一列赋值给self.x_star\n",
    "        self.t_star = torch.tensor(X_star[:,1:2], requires_grad=True).float().to(device) #将X_star的第二列赋值给self.t_star\n",
    "        self.u_star = torch.tensor(u_star).float().to(device) #将传入的u_star和v_star参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.u_star和self.v_star来访问这些值。\n",
    "        self.v_star = torch.tensor(v_star).float().to(device)\n",
    "        self.h_star = torch.tensor(h_star).float().to(device)\n",
    "        \n",
    "        # Initialize NNs \n",
    "        self.layers = layers #将传入的layers参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.layers来访问这些值。\n",
    "        \n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers,gamma,num_of_GP).to(device) #创建一个DNN类的实例，传入layers参数来实现神经网络的初始化，然后将这个实例移动到指定的设备上\n",
    "        \n",
    "\n",
    "\n",
    "        # optimizers: using the same settings，这里是使用pytorch库进行优化的部分\n",
    "        #创建优化器optimizer，使用LBFGS算法，具体每个参数意义见下方\n",
    "        self.optimizer_LBFGS = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), #要优化的参数，这里返回的是一个生成器，包含了self.dnn中的所有参数（神经网络权重、偏置以及两个新加的变量）\n",
    "            lr=1.0,  #学习率设置为1\n",
    "            max_iter=50000,  #最大迭代次数为50000\n",
    "            max_eval=50000,  #最大评估次数为50000\n",
    "            history_size=50, #历史大小为50，即用于计算Hessian矩阵近似的最近几步的信息\n",
    "            tolerance_grad=1e-5,  #优化的第一个停止条件，当梯度的L2范数小于1e-5时停止优化\n",
    "            tolerance_change=1.0 * np.finfo(float).eps, #优化的第二个停止条件，当优化的目标函数值的变化小于1.0 * np.finfo(float).eps时停止优化\n",
    "            line_search_fn=\"strong_wolfe\"       # 制定了用于一维搜索的方法，这里表示用强Wolfe条件\n",
    "        )\n",
    "        #创建第二个优化器，括号内为要优化的参数，使用Adam优化方法\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "                \n",
    "\n",
    "        self.iter = 0 #记录迭代次数 \n",
    "\n",
    "        self.loss_value = [] #创建一个空列表，用于存储损失值\n",
    "\n",
    "        self.test_error = [] #创建一个空列表，用于存储测试误差\n",
    "    \n",
    "    #这个函数与下面的net_uv函数功能相同，只是不计算梯度，因为在记录每个epoch的error时，使用with torch.no_grad()情况下调用net_uv函数会报错，不知道为啥？\n",
    "    def net_uv_error(self, x, t):  \n",
    "        uv = self.dnn(torch.cat([x, t], dim=1),self.lb,self.ub)  #（第一个参数将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量）调用DNN，根据两个参数权重和偏置，以及新得到的张量，计算神经网络的输出u\n",
    "        #将uv（是一个二维张量）的第一列赋值给u，第二列赋值给v\n",
    "        u=uv[:,0:1]\n",
    "        v=uv[:,1:2]\n",
    "\n",
    "        return u,v #返回神经网络的输出u和v，以及u关于x的梯度u_x和v关于x的梯度v_x\n",
    "\n",
    "\n",
    "    #pytorch中\n",
    "    #定义了一个名为net_u的函数/方法，用于计算神经网络的输出。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回神经网络的输出。     \n",
    "    def net_uv(self, x, t):  \n",
    "        uv = self.dnn(torch.cat([x, t], dim=1),self.lb,self.ub)  #（第一个参数将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量）调用DNN，根据两个参数权重和偏置，以及新得到的张量，计算神经网络的输出u\n",
    "        #将uv（是一个二维张量）的第一列赋值给u，第二列赋值给v\n",
    "        u=uv[:,0:1]\n",
    "        v=uv[:,1:2]\n",
    "\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v_x = torch.autograd.grad(\n",
    "            v, x, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        return u,v,u_x,v_x #返回神经网络的输出u和v，以及u关于x的梯度u_x和v关于x的梯度v_x\n",
    "\n",
    "\n",
    "    #定义了一个名为net_f的函数/方法，用于计算论文中的f。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回计算得到的f。\n",
    "    def net_f_uv(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "\n",
    "        u,v,u_x,v_x=self.net_uv(x,t) #调用上面的函数/方法，计算神经网络的输出（两个）以及输出关于输入x的梯度（两个）\n",
    "        \n",
    "        #计算u关于t的梯度，也就是u关于t的导数，这里使用了pytorch的自动求导功能\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t,  #输入的张量，要计算u关于t的导数\n",
    "            grad_outputs=torch.ones_like(u), #生成一个与u形状相同，所有元素均为1的张量，这个参数用于指定向量-雅可比积的像两部分\n",
    "            retain_graph=True, #表示计算完梯度之后保留计算图若需要多次计算梯度，则需要设置改参数为True\n",
    "            create_graph=True #创建梯度的计算图，使我们能够计算高阶导数\n",
    "        )[0] #这个函数的返回值是一个元组，其中包含了每个输入张量的梯度。这里只关心第一个输入张量u的梯度，所以我们使用[0]来获取这个梯度。？？？？又说只有一个梯度\n",
    "        v_t = torch.autograd.grad(\n",
    "            v, t, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v_xx = torch.autograd.grad(\n",
    "            v_x, x, \n",
    "            grad_outputs=torch.ones_like(v_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        f_u=u_t+0.5*v_xx+(u**2+v**2)*v    #计算f_u,定义见论文\n",
    "        f_v=v_t-0.5*u_xx-(u**2+v**2)*u   #计算f_v,定义见论文\n",
    "        return f_u, f_v  #返回计算得到的f_u和f_v\n",
    "\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.optimizer_LBFGS.zero_grad() #清除之前计算的梯度（在PyTorch中，梯度会累积，所以在每次新的优化迭代之前，我们需要清除之前的梯度）\n",
    "\n",
    "        u0_pred, v0_pred, _ , _ = self.net_uv(self.x0, self.t0) #是调用net_uv函数,将self.x0_tf和self.t0_tf作为参数传入,然后将返回的前两个结果赋值给self.u0_pred和self.v0_pred。后两个_是Python惯用法，表示不关心net_uv函数返回的后两个结果。\n",
    "        u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.x_lb, self.t_lb) #同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\n",
    "        u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.x_ub, self.t_ub) #同上\n",
    "        f_u_pred, f_v_pred = self.net_f_uv(self.x_f, self.t_f) #调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\n",
    "\n",
    "        loss = torch.mean((self.u0 - u0_pred) ** 2)  + \\\n",
    "                    torch.mean((self.v0 - v0_pred) ** 2) + \\\n",
    "                    torch.mean((u_lb_pred - u_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_lb_pred - v_ub_pred) ** 2) + \\\n",
    "                    torch.mean((u_x_lb_pred - u_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_x_lb_pred - v_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean(f_u_pred ** 2) + \\\n",
    "                    torch.mean(f_v_pred ** 2)\n",
    "        loss.backward() #被调用以计算损失函数关于神经网络参数的梯度。这个梯度将被用于优化器来更新神经网络参数\n",
    "        \n",
    "        self.iter += 1 #每调用一次损失函数，迭代次数加1\n",
    "\n",
    "\n",
    "        #record the loss value\n",
    "        self.loss_value.append(loss) #将计算得到的loss值添加到self.loss_value列表中\n",
    "\n",
    "        #record the test error\\\n",
    "        with torch.no_grad():\n",
    "            u_real_pred, v_real_pred= self.net_uv_error(self.x_star, self.t_star)\n",
    "            h_real_pred = torch.sqrt(u_real_pred**2 + v_real_pred**2)\n",
    "\n",
    "        error_u_test = torch.norm(self.u_star - u_real_pred, 2) / torch.norm(self.u_star, 2)\n",
    "        error_v_test = torch.norm(self.v_star - v_real_pred, 2) / torch.norm(self.v_star, 2)\n",
    "        error_h_test = torch.norm(self.h_star - h_real_pred, 2) / torch.norm(self.h_star, 2)\n",
    "        \n",
    "        self.test_error.append(torch.tensor([error_u_test.item(), error_v_test.item(), error_h_test.item()]))\n",
    "        \n",
    "\n",
    "        return loss #返回loss\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #定义了一个名为train的函数/方法，用于训练神经网络。这个方法接受一个参数nIter，表示训练的迭代次数。\n",
    "    def train(self, nIter, nIterLBFGS):\n",
    "        self.dnn.train()#将神经网络设置为训练模式而不是评估模式\n",
    "\n",
    "        #先使用Adam优化器优化nIter次\n",
    "        for epoch in tqdm(range(nIter), desc='Adam'):\n",
    "            u0_pred, v0_pred, _ , _ = self.net_uv(self.x0, self.t0) #是调用net_uv函数,将self.x0_tf和self.t0_tf作为参数传入,然后将返回的前两个结果赋值给self.u0_pred和self.v0_pred。后两个_是Python惯用法，表示不关心net_uv函数返回的后两个结果。\n",
    "            u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.x_lb, self.t_lb) #同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\n",
    "            u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.x_ub, self.t_ub) #同上\n",
    "            f_u_pred, f_v_pred = self.net_f_uv(self.x_f, self.t_f) #调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\n",
    "\n",
    "            loss = torch.mean((self.u0 - u0_pred) ** 2)  + \\\n",
    "                    torch.mean((self.v0 - v0_pred) ** 2) + \\\n",
    "                    torch.mean((u_lb_pred - u_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_lb_pred - v_ub_pred) ** 2) + \\\n",
    "                    torch.mean((u_x_lb_pred - u_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_x_lb_pred - v_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean(f_u_pred ** 2) + \\\n",
    "                    torch.mean(f_v_pred ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad() #清除该优化器之前计算的梯度（在PyTorch中，梯度会累积，所以在每次新的优化迭代之前，我们需要清除之前的梯度）\n",
    "            loss.backward() #被调用以计算损失函数关于神经网络参数的梯度。这个梯度将被用于优化器来更新神经网络参数\n",
    "            self.optimizer_Adam.step()  #使用之前的优化器self.optimizer_Adam，调用step方法(执行一步优化算法)，传入损失函数self.loss_func，进行优化\n",
    "            \n",
    "\n",
    "            #record the loss value\n",
    "            self.loss_value.append(loss) #将计算得到的loss值添加到self.loss_value列表中\n",
    "            \n",
    "\n",
    "            #record the test error\n",
    "            with torch.no_grad():\n",
    "                u_real_pred, v_real_pred = self.net_uv_error(self.x_star, self.t_star)\n",
    "                h_real_pred = torch.sqrt(u_real_pred**2 + v_real_pred**2)\n",
    "\n",
    "            error_u_test = torch.norm(self.u_star - u_real_pred, 2) / torch.norm(self.u_star, 2)\n",
    "            error_v_test = torch.norm(self.v_star - v_real_pred, 2) / torch.norm(self.v_star, 2)\n",
    "            error_h_test = torch.norm(self.h_star - h_real_pred, 2) / torch.norm(self.h_star, 2)\n",
    "            \n",
    "            self.test_error.append(torch.tensor([error_u_test.item(), error_v_test.item(), error_h_test.item()]))\n",
    "\n",
    "\n",
    "\n",
    "        #Backward the optimize，使用LBFGS优化器进一步，注意这里虽然迭代了500次，但其实使用LBFGS优化器优化的次数不止500次\n",
    "        for i in tqdm(range(nIterLBFGS), desc='LBFGS'):\n",
    "            self.optimizer_LBFGS.step(self.loss_func)  #使用之前的优化器self.optimizer，调用step方法(执行一步优化算法)，传入计算损失函数的方法self.loss_func，进行优化   \n",
    "\n",
    "                                    \n",
    "    #定义了一个名为predict的函数/方法，用于预测神经网络的输出。这个方法接受一个参数X_star，表示输入数据。最后返回预测的两个输出和两个输出的梯度。\n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device) #从输入中得到x和t（第一列和第二列），是张量，需要计算梯度，转换为浮点数类型，并将张量移动到指定设备上\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval() #将神经网络切换为评估模式\n",
    "        u, v, _, _ = self.net_uv(x, t) #调用之前定义的函数得到神经网络的输出u,以及f\n",
    "        f_u, f_v = self.net_f_uv(x, t) \n",
    "\n",
    "        u = u.detach().cpu().numpy() #将张量u和v先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "        v = v.detach().cpu().numpy()\n",
    "        f_u = f_u.detach().cpu().numpy()\n",
    "        f_v = f_v.detach().cpu().numpy()\n",
    "        return u, v, f_u, f_v \n",
    "    \n",
    "\n",
    "    def net_for_hidden_features(self, X):\n",
    "        self.dnn.eval() #将神经网络切换为评估模式\n",
    "        # x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        X = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "\n",
    "\n",
    "        H,Fai = self.dnn.dnn_for_hidden_features(X, self.lb, self.ub)\n",
    "        H = H.detach().cpu().numpy()\n",
    "        Fai = Fai.detach().cpu().numpy()\n",
    "        return H,Fai\n",
    "\n",
    "\n",
    "\n",
    "    # #定义函数获得隐藏层的输出\n",
    "    # def hidden_predict(self, x,t):\n",
    "    #     x = torch.tensor(x, requires_grad=True).float().to(device) #从输入中得到x和t（第一列和第二列），是张量，需要计算梯度，转换为浮点数类型，并将张量移动到指定设备上\n",
    "    #     t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "    #     self.dnn.eval() #将神经网络切换为评估模式\n",
    "    #     hidden_output = self.dnn.hidden_output(torch.cat([x, t], dim=1)) #调用上一个神经网络类中的hidden_output方法，得到最后一个隐藏层的输出\n",
    "    #     hidden_output_x = hidden_output[:, 0] #将输出的第一列赋值给hidden_output_x\n",
    "    #     hidden_output_t = hidden_output[:, 1] #将输出的第二列赋值给hidden_output_t\n",
    "    #     hidden_output_x = hidden_output_x.detach().cpu().numpy() #将张量hidden_output_x和hidden_output_t先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "    #     hidden_output_t = hidden_output_t.detach().cpu().numpy() #将张量hidden_output_x和hidden_output_t先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "    #     return hidden_output_x, hidden_output_t #返回隐藏层的输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义设置随机数种子的函数，第一个参数seed表示种子；第二个参数用来设置CUDA的卷积操作是否确定性，默认为False，表示没有确定性\n",
    "def set_seed(seed):\n",
    "    # torch.manual_seed(seed) #设置pytorch的CPU随机数生成器的种子\n",
    "    # torch.cuda.manual_seed_all(seed) #设置putorch的所有GPU随机数生成器的种子\n",
    "    # np.random.seed(seed) #设置numpy的随机数生成器的种子\n",
    "    # random.seed(seed) #设置python的内置随机数生成器的种子\n",
    "    # torch.backends.cudnn.deterministic = deterministic #True会让CUDA的卷积操作变得确定性，即对于相同的输入，每次运行会得到相同的结果，False则相反\n",
    "    \"\"\"\n",
    "    设置PyTorch的随机种子, 用于生成随机数. 通过设置相同的种子, 可以确保每次运行时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置PyTorch在所有可用的CUDA设备上的随机种子. 如果在使用GPU进行计算, 这个设置可以确保在不同的GPU上生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置PyTorch在当前CUDA设备上的随机种子. 它与上一行代码的作用类似, 但只影响当前设备\n",
    "    \"\"\"\n",
    "    torch.cuda.manual_seed(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置NumPy的随机种子, 用于生成随机数. 通过设置相同的种子，可以确保在使用NumPy的随机函数时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    设置Python内置的随机函数的种子. Python的random模块提供了许多随机函数, 包括生成随机数、打乱列表等. 通过设置相同的种子, 可以确保使用这些随机函数时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    设置Python的哈希种子 (哈希函数被广泛用于数据结构 (如字典和集合) 的实现，以及一些内部操作 (如查找和比较)). 通过设置相同的种子, 可以确保在不同的运行中生成的哈希结果相同\n",
    "    \"\"\"\n",
    "    # os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    该设置确保每次运行代码时, cuDNN的计算结果是确定性的, 即相同的输入会产生相同的输出, 这是通过禁用一些非确定性的算法来实现的, 例如在卷积操作中使用的算法. 这样做可以保证模型的训练和推理在相同的硬件和软件环境下是可复现的, 即每次运行代码时的结果都相同. 但是, 这可能会导致一些性能上的损失, 因为禁用了一些优化的非确定性算法\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \"\"\"\n",
    "    该设置禁用了cuDNN的自动优化过程. 当它被设置为False时, PyTorch不会在每次运行时重新寻找最优的算法配置, 而是使用固定的算法配置. 这样做可以确保每次运行代码时的性能是一致的, 但可能会导致一些性能上的损失\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/lcytorch/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Adam:  69%|███████████████████████████████████████████████████████████████████▉                               | 6864/10000 [03:26<01:34, 33.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()       \n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#训练模型50000次         \u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnIter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIterLBFGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#获取当前时间并减去start_time，得到训练时间并赋值给elapsed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time                \n",
      "Cell \u001b[0;32mIn[5], line 199\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[0;34m(self, nIter, nIterLBFGS)\u001b[0m\n\u001b[1;32m    197\u001b[0m u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_uv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_lb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_lb) \u001b[38;5;66;03m#同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\u001b[39;00m\n\u001b[1;32m    198\u001b[0m u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_uv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_ub, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_ub) \u001b[38;5;66;03m#同上\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m f_u_pred, f_v_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_f_uv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_f\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\u001b[39;00m\n\u001b[1;32m    201\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu0 \u001b[38;5;241m-\u001b[39m u0_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    202\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv0 \u001b[38;5;241m-\u001b[39m v0_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    203\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmean((u_lb_pred \u001b[38;5;241m-\u001b[39m u_ub_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmean(f_u_pred \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    208\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmean(f_v_pred \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 125\u001b[0m, in \u001b[0;36mPhysicsInformedNN.net_f_uv\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#计算u关于t的梯度，也就是u关于t的导数，这里使用了pytorch的自动求导功能\u001b[39;00m\n\u001b[1;32m    119\u001b[0m u_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    120\u001b[0m     u, t,  \u001b[38;5;66;03m#输入的张量，要计算u关于t的导数\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u), \u001b[38;5;66;03m#生成一个与u形状相同，所有元素均为1的张量，这个参数用于指定向量-雅可比积的像两部分\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m#表示计算完梯度之后保留计算图若需要多次计算梯度，则需要设置改参数为True\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m#创建梯度的计算图，使我们能够计算高阶导数\u001b[39;00m\n\u001b[1;32m    124\u001b[0m )[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#这个函数的返回值是一个元组，其中包含了每个输入张量的梯度。这里只关心第一个输入张量u的梯度，所以我们使用[0]来获取这个梯度。？？？？又说只有一个梯度\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m v_t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m u_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    132\u001b[0m     u_x, x, \n\u001b[1;32m    133\u001b[0m     grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u_x),\n\u001b[1;32m    134\u001b[0m     retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    135\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    136\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    137\u001b[0m v_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    138\u001b[0m     v_x, x, \n\u001b[1;32m    139\u001b[0m     grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(v_x),\n\u001b[1;32m    140\u001b[0m     retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/lcytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:436\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    432\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    433\u001b[0m         grad_outputs_\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    447\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    449\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/lcytorch/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # seeds = [0, 1, 12, 21, 123, 321, 1234, 4321, 12345, 54321, 123456, 654321, 10, 210, 3210, 43210, 543210, 6543210, 1234567, 76543210] #生成10个随机种子\n",
    "seed = 1234\n",
    "set_seed(seed) #设置随机数种子\n",
    "\n",
    "#设置噪声值为0 \n",
    "noise = 0.0        \n",
    "\n",
    "# Doman bounds，定义两个一维数组lb和ub，问题域是一个二维空间，其中 x 的范围是 -5 到 5，t 的范围是 0 到 π/2(竖着的)\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "#定义三个整数，分别表示初始条件点数量、边界条件点数量和配位点的数量（这些点用于训练神经网络）\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 20000\n",
    "\n",
    "gamma = 5.0 #GP参数\n",
    "num_of_GP = 1024 #GP参数\n",
    "\n",
    "\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [2, 100, 100, 100, 100, 2]\n",
    "#读取名为NLS.mat的Matlab文件，文件中的数据存储在data变量中。这里的路径也要随着设备的情况修改    \n",
    "data = scipy.io.loadmat('../data/NLS.mat')\n",
    "#从data字典中取出变量tt和x的值，并转换为一维数组（flatten方法），最后tongg[:,None]将一维数组转换为二维数组\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu'] #从data字典中取出变量uu的值，并赋值给Exact\n",
    "Exact_u = np.real(Exact)  #取Exact的实部，赋值给Exact_u\n",
    "Exact_v = np.imag(Exact)  #取Exact的虚部，赋值给Exact_v\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2) #计算复数uu的|uu|\n",
    "#生成一个二位网络，X和T是输出的二维数组\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #X_star是一个二维数组，其中第一列是X的展平，第二列是T的展平\n",
    "u_star = Exact_u.T.flatten()[:,None] #先对Exact_u进行转置，然后使用flatten方法将其转换为一维数组，最后使用[:,None]将其转换为二维数组\n",
    "v_star = Exact_v.T.flatten()[:,None] #同上，比如Exact_v是m*n二维数组，Exact_v.T是n*m二维数组，Exact_v.T.flatten()是一个长度为n*m的一维数组，Exact_v.T.flatten()[:,None]是一个(n*m)*1的三维数组\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "#上面五行代码的意义见Numpy库的索引的介绍\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "#从0~数组x的行数(256)中随机选择N0个数，replace=False表示不允许重复选择，最后将这N0个数赋值给idx_x\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "#从x中选择N0个对应的行(idx_x对应的行)，最后将这N0行赋值给x0\n",
    "x0 = x[idx_x,:]\n",
    "#从Exact_u中选择N0个对应的行(idx_x对应的行)的第一列元素，最后将这N0个元素赋值给u0\n",
    "u0 = Exact_u[idx_x,0:1]\n",
    "v0 = Exact_v[idx_x,0:1]\n",
    "#从0~数组t的行数中随机选择N_b个数，replace=False表示不允许重复选择，最后将这N_b个数赋值给idx_t\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "#从t中选择N_b个对应的行(idx_t对应的行)，最后将这N_b行赋值给tb\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i = 0 #初始化i为0\n",
    "\n",
    "\n",
    "\n",
    "nIter = 10000 #设置迭代次数为10000\n",
    "nIterLBFGS = 0 #设置LBFGS迭代次数为500\n",
    "\n",
    "\n",
    "X_f = lb + (ub-lb)*lhs(2, N_f) #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（$N_f$，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本$X_f$。\n",
    "\n",
    "\n",
    "#开始训练\n",
    "\n",
    "\n",
    "#创建PINN模型并输入各种参数        \n",
    "model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub, X_star, u_star, v_star, h_star, gamma, num_of_GP)\n",
    "#获取当前时间并赋值给start_time          \n",
    "start_time = time.time()       \n",
    "#训练模型50000次         \n",
    "model.train(nIter, nIterLBFGS)\n",
    "#获取当前时间并减去start_time，得到训练时间并赋值给elapsed\n",
    "elapsed = time.time() - start_time                \n",
    "#打印训练所需时间\n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "\n",
    "\n",
    "#每一个seed训练好之后，用训练好的模型进行预测，得到测试误差\n",
    "\n",
    "#用训练好的模型进行预测，返回四个值（均为数组）    \n",
    "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
    "#计算u_pred和v_pred的模（平方和的平方根），赋值给h_pred\n",
    "h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
    "#计算误差（基于2范数）        \n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
    "error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
    "#打印误差\n",
    "print('Error u: %e' % (error_u))\n",
    "print('Error v: %e' % (error_v))\n",
    "print('Error h: %e' % (error_h))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#使用griddata函数将X_star、u_pred、v_pred和h_pred插值到网格上，得到U_pred、V_pred和H_pred\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
    "H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
    "#同上，使用griddata函数将X_star、f_u_pred和f_v_pred插值到网格上，得到FU_pred和FV_pred\n",
    "FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
    "FV_pred = griddata(X_star, f_v_pred.flatten(), (X, T), method='cubic')     \n",
    "\n",
    "H_star = griddata(X_star, h_star.flatten(), (X, T), method='cubic') #同上，将X_star和h_star插值到网格上，得到H_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloss = torch.stack(model.loss_value).cpu().detach().numpy()\n",
    "\n",
    "testerror = torch.stack(model.test_error).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainloss))\n",
    "print(trainloss)\n",
    "# 定义图例的标签\n",
    "labels = [\"Grid\"]\n",
    "\n",
    "# 创建一个新的图形\n",
    "plt.figure()\n",
    "\n",
    "# # 遍历所有的loss数据和标签\n",
    "# for loss, label in zip(trainloss, labels):\n",
    "#     # 绘制每个loss数据，并使用标签作为图例的条目\n",
    "#     plt.plot(loss, label=label)\n",
    "\n",
    "plt.plot(trainloss, label='train loss')\n",
    "\n",
    "# 添加图例，放在图像外\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "plt.xlim([0,10000]) # 设置x轴的范围\n",
    "plt.yscale('log') #设置y轴为对数尺度，这样即使列表中有一些非常大的值，也不会影响其他值的可视化\n",
    "plt.xlabel('Iteration') # 设置x轴的标签\n",
    "plt.ylabel('Loss') # 设置y轴的标签\n",
    "plt.title('Loss vs. Iteration based on different methods of sampling') # 设置图形的标题\n",
    "# 显示图形\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(trainloss))\n",
    "# print(trainloss)\n",
    "# 定义图例的标签\n",
    "# labels = [\"Grid\"]\n",
    "\n",
    "# 创建一个新的图形\n",
    "plt.figure()\n",
    "\n",
    "u_error = testerror[:,0]\n",
    "v_error = testerror[:,1]\n",
    "h_error = testerror[:,2]\n",
    "\n",
    "plt.plot(u_error, label='test error of u')\n",
    "plt.plot(v_error, label='test error of v')\n",
    "plt.plot(h_error, label='test error of h')\n",
    "\n",
    "\n",
    "# 添加图例，放在图像外\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "plt.xlim([0,10000]) # 设置x轴的范围\n",
    "plt.yscale('log') #设置y轴为对数尺度，这样即使列表中有一些非常大的值，也不会影响其他值的可视化\n",
    "plt.xlabel('Iteration') # 设置x轴的标签\n",
    "plt.ylabel('Error') # 设置y轴的标签\n",
    "plt.title('Error vs. Iteration based on different methods of sampling') # 设置图形的标题\n",
    "# 显示图形\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###### Row 0: u(t,x) ##################    \n",
    "\n",
    "fig = plt.figure(figsize=(12, 7)) #这里figure是一个figure对象，是一个图形窗口，代表整个图形，设置图形大小为宽9英寸高5 英寸\n",
    "ax = fig.add_subplot(111) #在图形的1*1网格的第一个位置添加一个子图\n",
    "\n",
    "#绘制热图\n",
    "h = ax.imshow(H_star.T, interpolation='nearest', cmap='YlGnBu', \n",
    "                extent=[lb[1], ub[1], lb[0], ub[0]], \n",
    "                origin='lower', aspect='auto')  #imshow函数用于显示图像，接受一些参数，第一个参数是图像数据，这里是H_pred的转置；第二个参数是插值方法（用于在像素之间插入新的像素），这里是最邻近插值；\n",
    "                                                #第三个参数是颜色映射，这里是从黄色Yl到绿色Gn再到蓝色Bu；第四个参数是图像的范围，这里lb和ub分别是数据的下界和上界；第五个参数是图像的原点位置，这里表示原点在右下角；第六个参数是图像的纵横比，这里表示调整横纵比以填充整个axes对象\n",
    "                                                #最后的结果返回一个axesimage对象，也就是h，可以通过这个对象进一步设置图像的属性\n",
    "divider = make_axes_locatable(ax)  #使用 make_axes_locatable 函数创建了一个 AxesDivider 对象。这个函数接受一个 Axes 对象作为参数，返回一个 AxesDivider 对象。AxesDivider 对象可以用来管理子图的布局，特别是当你需要在一个图的旁边添加另一个图时。\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05) #使用append_axes方法在原始轴的右侧添加了一个新的轴。append_axes 方法接受三个参数：位置（\"right\"）、大小（\"5%\"）和间距（0.05）。在原始轴的右侧添加了一个新的轴，新轴的大小是原始轴的 5%，新轴与原始轴之间的间距是 0.05 英寸\n",
    "cbar = fig.colorbar(h, cax=cax)#使用colorbar方法在新轴上添加了一个颜色条。colorbar 方法接受两个参数：axesimage 对象（h）和新轴（cax）。并记这个颜色条的名字是cbar\n",
    "cbar.ax.tick_params(labelsize=15) #设置颜色条刻度标签大小，这里bar是之前定义的\n",
    "\n",
    "ax.plot(\n",
    "    X_f[:,1], \n",
    "    X_f[:,0], \n",
    "    'ko', label = 'Data (%d points)' % (X_f.shape[0]), \n",
    "    markersize = 2,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ") #在ax上绘制散点图，前两个参数是散点的x坐标和y坐标；kx表示黑色的x（散点形状是x），label是散点的标签，clip_on表示散点可以绘制在轴的边界外\n",
    "  #新加了一个alpha=1.0用来设置标记的透明度，1.0表示完全不透明  \n",
    "\n",
    "# #绘制三条虚线\n",
    "# line = np.linspace(x.min(), x.max(), 2)[:,None] #生成了一个包含2个等间距的数值的数组，这些数值在 x.min() 到 x.max() 之间。[:,None] 是一个索引操作，用于将一维数组转换为二维数组。这里其实就是[-5;5]\n",
    "# #第一个参数是虚线的x坐标，line是虚线y的坐标，第三个参数是虚线的样式，k表示黑色，--表示虚线，最后一个参数表示虚线的参数是1\n",
    "# ax.plot(t[75]*np.ones((2,1)),line,'k--',linewidth=1) \n",
    "# ax.plot(t[100]*np.ones((2,1)),line,'k--',linewidth=1)\n",
    "# ax.plot(t[125]*np.ones((2,1)),line,'k--',linewidth=1)    \n",
    "\n",
    "#设置标签\n",
    "#设置ax子图的x轴的标签为t，y轴的标签为x。这里$t$和$x$是latex格式的文本，用于生成数学公式\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "#设置子图ax的图例，第一个参数用来设置图例的位置，这里表示图例放在上方中央;第二个参数用来设置图例的锚点，接受一个元组，表示x和y坐标，表示把图例的锚点设置在x坐标为0.9，y坐标为-0.05处;frameon=False表示不显示图例的边框;第三个参数设置图例的列数，分为5列；最后一个参数设置图例属性，接受字典，这里表示设置图例字体大小为15；最后返回的leg是一个legend对象，表示图形的图例\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "#    plt.setp(leg.get_texts(), color='w')   #用来设置图例中文本的颜色，这里是白色，取消注释后文本会变为白色\n",
    "ax.set_title('$|h(t,x)|$', fontsize = 20) #设置子图ax的标题为$|h(t,x)|$，表示latex格式的文本，用于生成数学公式，fontsize=10表示字体大小为10\n",
    "ax.tick_params(labelsize=15)#用来设置刻度标签的大小。`tick_params`是`Axes`对象的一个方法，可以用来设置刻度线的属性。在这里，`labelsize=15` 是用来设置刻度标签的字体大小的。\n",
    "\n",
    "\n",
    "#显示图片\n",
    "plt.show() #显示所有打开的图形的函数\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Row 1: h(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1,3) #创建一个1×3的网络，用于存放子图\n",
    "\n",
    "ax = plt.subplot(gs1[0,0])  #在gs1[0,0]指定的位置，也就是网格的第一行第一列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "#绘制了两条线，一条表示精确值，一条表示预测值\n",
    "ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')      #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction') #同上\n",
    "#设置ax子图的x轴的标签为x，y轴的标签为|h(t,x)|。这里$x$和$|h(t,x)|$是latex格式的文本，用于生成数学公式\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$|h(t,x)|$')    \n",
    "#设置子图的标题，几个子图标题随着t的变化而变化，字体大小为10 \n",
    "ax.set_title('$t=%.2f$' % (t[75]), fontsize = 10)\n",
    "ax.axis('square') #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1]) #第一个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1]) #第一个子图的y轴范围是-0.1到5.1\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1]) #在gs1[0,1]指定的位置，也就是网格的第一行第二列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "#绘制了两条线，一条表示精确值，一条表示预测值\n",
    "ax.plot(x,Exact_h[:,100],'b-', linewidth = 2, label = 'Exact')        #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[100,:],'r--', linewidth = 2, label = 'Prediction')   #同上\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$|h(t,x)|$') #设置子图的y轴的标签为|h(t,x)|\n",
    "ax.axis('square')   #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1])     #第二个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1])     #第二个子图的y轴范围是-0.1到5.1\n",
    "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)        #设置第二个子图的标题，标题随着t的变化而变化，字体大小为10\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)  #设置第二个子图的图例，loc='upper center'表示图例的位置是上方中心，bbox_to_anchor=(0.5,-0.8)表示图例的中心位置是在子图的中间偏下方0.8的位置，ncol=5表示图例的列数是5，frameon=False表示不显示图例的边框\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2]) #在gs1[0,2]指定的位置，也就是网格的第一行第三列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')        #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')    #同上\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$|h(t,x)|$') #设置子图的y轴的标签为|h(t,x)|\n",
    "ax.axis('square')    #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1])    #第三个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1])    #第三个子图的y轴范围是-0.1到5.1\n",
    "ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)    #设置第三个子图的标题，标题随着t的变化而变化，字体大小为10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 创建一个包含两个子图的图形\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5))  # 1行2列的子图，图形大小为宽18英寸高5英寸\n",
    "\n",
    "# 绘制第一个子图，展示预测数据 H_pred\n",
    "ax1 = axs[1]\n",
    "h1 = ax1.imshow(H_pred.T, interpolation='nearest', cmap='YlGnBu', \n",
    "                extent=[lb[1], ub[1], lb[0], ub[0]], \n",
    "                origin='lower', aspect='auto')\n",
    "ax1.set_xlabel('$t$')\n",
    "ax1.set_ylabel('$x$')\n",
    "ax1.set_title('$Prediction |h(t,x)|$', fontsize=10)\n",
    "\n",
    "# 绘制第二个子图，展示真实数据 H_star\n",
    "ax2 = axs[0]\n",
    "h2 = ax2.imshow(H_star.T, interpolation='nearest', cmap='YlGnBu', \n",
    "                extent=[lb[1], ub[1], lb[0], ub[0]], \n",
    "                origin='lower', aspect='auto')\n",
    "ax2.set_xlabel('$t$')\n",
    "ax2.set_ylabel('$x$')\n",
    "ax2.set_title('$True |h(t,x)|$', fontsize=10)\n",
    "\n",
    "# 创建一个共享的颜色条\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h2, cax=cax)\n",
    "\n",
    "\n",
    "# 在图形底部添加误差值\n",
    "\n",
    "fig.text(0.5, -0.05, f'Error: {error_h}', ha='center', fontsize=12)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "X_know = np.concatenate((X0, X_lb,X_ub,X_f),0) #X_know=[X0;X_lb;X_ub;X_f_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用训练好的模型进行预测，返回四个值（均为数组） \n",
    "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_know)\n",
    "f_know = u_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hidden_pred_small, Fai_pred_small = model.net_for_hidden_features(X_know)\n",
    "X =Hidden_pred_small\n",
    "Hidden_pred_all, Fai_pred_all = model.net_for_hidden_features(X_star )\n",
    "X_xing = Hidden_pred_all\n",
    "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
    "Prediction_all =  u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 作图1\n",
    "loc_ind=  200\n",
    "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star[256*loc_ind+0:256*loc_ind+256,:])\n",
    "u_reall = u_star[256*loc_ind+0:256*loc_ind+256,:]\n",
    "x_loaction = np.linspace(-5.0,5.0,256)\n",
    "plt.style.use('default')\n",
    "plt.figure()\n",
    "plt.plot(x_loaction,u_pred)\n",
    "plt.plot(x_loaction,u_reall)\n",
    "plt.legend(['prediction','actual value'], loc = 'best') \n",
    "# plt.xlim(-5, 5)\n",
    "# plt.ylim(-0.10, 1.8)\n",
    "np.max(np.abs(u_reall-u_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "X_know = np.concatenate((X0, X_lb,X_ub,X_f),0) #X_know=[X0;X_lb;X_ub;X_f_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用训练好的模型进行预测，返回四个值（均为数组） \n",
    "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_know)\n",
    "f_know = u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_know.shape)\n",
    "# print(X_know)\n",
    "print(u_pred.shape)\n",
    "# print(u_pred)\n",
    "print(f_u_pred.shape)\n",
    "# print(f_u_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star) #测试点的输出u和f\n",
    "prediction_all = u_pred #测试点的预测u值，即神经网络的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u_pred.shape)\n",
    "print(u_pred)\n",
    "print(f_u_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer of X_konw\n",
    "Hidden_pred_small, Fai_pred_small = model.net_for_hidden_features(X_know) #获得高斯过程层的在训练点的两个输出\n",
    "X = Hidden_pred_small #X代表训练点的高斯过程层的常规输出\n",
    "hidden_feat_know = Fai_pred_small #hidden_feat_know代表训练点的高斯过程层的特殊处理输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Hidden_pred_small.shape)\n",
    "print(Hidden_pred_small)\n",
    "print(Fai_pred_small.shape)\n",
    "print(Fai_pred_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden layer for new data\n",
    "Hidden_pred_all, Fai_pred_all = model.net_for_hidden_features(X_star) #获得高斯过程层的在测试点的两个输出\n",
    "X_xing = Hidden_pred_all #X_xing代表测试点的高斯过程层的常规输出\n",
    "hidden_feat_all = Fai_pred_all #hidden_feat_all代表测试点的高斯过程层的特殊处理输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Hidden_pred_all.shape)\n",
    "print(Hidden_pred_all)    \n",
    "print(Fai_pred_all.shape)\n",
    "print(Fai_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取X_xing（测试点的高斯过程层的常规输出）的协方差矩阵Cov_xing_xing\n",
    "n_xing = X_xing.shape[0]\n",
    "Cov_xing_xing = np.zeros(shape=(n_xing, n_xing), dtype=float)\n",
    "for i in tqdm(range(n_xing), desc='waibu'):\n",
    "    for j in range(n_xing):\n",
    "        xi = X_xing[i:i+1,:]\n",
    "        xj = X_xing[j:j+1,:]\n",
    "        dij = np.sum((xi-xj)**2)\n",
    "        Cov_xing_xing[i][j] = np.exp(-gamma*dij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_xing)\n",
    "print(Cov_xing_xing.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取X（训练点的高斯过程层的常规输出）的协方差矩阵Cov_X_X\n",
    "n = X.shape[0]\n",
    "Cov_X_X = np.zeros(shape=(n, n), dtype=float)\n",
    "for i in tqdm(range(n), desc='waibu'):\n",
    "    for j in range(n):\n",
    "        xi = X[i:i+1,:]\n",
    "        xj = X[j:j+1,:]\n",
    "        dij = np.sum((xi-xj)**2)\n",
    "        Cov_X_X[i][j] = np.exp(-gamma*dij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n)\n",
    "print(Cov_X_X.shape)\n",
    "print(Cov_X_X)\n",
    "print(np.zeros(shape=(n,n), dtype=float).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算X_xing（测试点的高斯过程层的常规输出）和X（训练点的高斯过程层的常规输出）的协方差矩阵Cov_xing_X\n",
    "Cov_xing_X = np.zeros(shape=(n_xing, n), dtype=float)\n",
    "for i in tqdm(range(n_xing), desc='waibu'):\n",
    "    for j in range(n):\n",
    "        xi = X_xing[i:i+1,:]\n",
    "        xj = X[j:j+1,:]\n",
    "        dij = np.sum((xi-xj)**2)\n",
    "        Cov_xing_X[i][j] = np.exp(-gamma*dij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Cov_xing_X.shape)\n",
    "# print(Cov_xing_X)\n",
    "# print(np.zeros(shape=(n_xing,n), dtype=float).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将X_xing（测试点的高斯过程层的常规输出）和X（训练点的高斯过程层的常规输出）的协方差矩阵Cov_xing_X进行转置\n",
    "Cov_X_xing = Cov_xing_X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_orig = Cov_X_X #X（训练点的高斯过程层的常规输出）的协方差矩阵Cov_X_X，将其赋值给rho_orig\n",
    "xishu = 100000000 #一个常数，用于放大协方差矩阵的元素，避免数值计算精度问题\n",
    "rho = rho_orig*xishu # 将X（训练点的高斯过程层的常规输出）的协方差矩阵Cov_X_X（现在是rho_orig）放大xishu倍，得到rho\n",
    "u, s, v = np.linalg.svd(rho) #对放大后的协方差矩阵rho进行奇异值分解，得到矩阵u，奇异值向量s和矩阵v，具体rho = u * np.diag(s) * v.T\n",
    "inv_s=np.linalg.inv(np.diag(s)) #计算奇异值向量s对角矩阵的逆矩阵inv_s\n",
    "t1= np.matmul(v.T, inv_s) #将矩阵v的转置与逆奇异值矩阵inv_s相乘，得到中间结果 t1\n",
    "t2= np.matmul(t1, u.T) #将中间结果t1与矩阵u的转置相乘，得到中间结果t2\n",
    "# Check whether the obtained inverse matrix is correct. If it is the identity matrix, it means there is no problem.\n",
    "t3= np.matmul(t2, rho) #将中间结果t2与矩阵rho相乘，得到结果t3。如果t2是rho的正确逆矩阵，那么t3应该是一个单位矩阵（即对角线元素为1，其余元素为0）\n",
    "# Inverse of covariance matrix\n",
    "inv_rho = t2*xishu  #将中间结果t2的每个元素乘以放大常数xishu，得到原始协方差矩阵rho_orig（即X（训练点的高斯过程层的常规输出）的协方差矩阵Cov_X_X）的逆矩阵 inv_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "xuedingeshuju = {\n",
    "    \"X0\": X0,\n",
    "    \"X_lb\": X_lb,\n",
    "    \"X_ub\": X_ub,\n",
    "    \"X_know\": X_know,\n",
    "    \"f_know\": f_know,\n",
    "    \"prediction_all\": prediction_all,\n",
    "    \"X\": X,\n",
    "    \"hidden_feat_know\": hidden_feat_know,\n",
    "    \"X_xing\": X_xing,\n",
    "    \"hidden_feat_all\": hidden_feat_all,\n",
    "    \"n_xing\": n_xing,\n",
    "    \"Cov_xing_xing\": Cov_xing_xing,\n",
    "    \"n\": n,\n",
    "    \"Cov_X_X\": Cov_X_X,\n",
    "    \"Cov_xing_X\": Cov_xing_X,\n",
    "    \"Cov_X_xing\": Cov_X_xing,\n",
    "}\n",
    "\n",
    "# 将字典存储到文件中\n",
    "joblib_file = 'xuedingeshuju.pkl'\n",
    "joblib.dump(xuedingeshuju, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m joblib_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxuedingeshuju.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 从文件中读取字典\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m loaded_dict \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoblib_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 验证读取的字典\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(loaded_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/lcytorch/lib/python3.10/site-packages/joblib/numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/anaconda3/envs/lcytorch/lib/python3.10/site-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lcytorch/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mKeyError\u001b[0m: 64"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# 将字典存储到文件中\n",
    "joblib_file = 'xuedingeshuju.pkl'\n",
    "\n",
    "# 从文件中读取字典\n",
    "loaded_dict = joblib.load(joblib_file)\n",
    "\n",
    "# 验证读取的字典\n",
    "print(loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = Cov_xing_xing - Cov_xing_X * inv_rho * Cov_X_xing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "lcytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
